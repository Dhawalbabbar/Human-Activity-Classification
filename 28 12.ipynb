{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "\n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return y_ - 1\n",
    "\n",
    "def one_hot(y_):\n",
    "    \"\"\"\n",
    "    Function to encode output labels from number indexes.\n",
    "    E.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \"\"\"\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"\n",
    "    define a class to store parameters,\n",
    "    the input should be feature mat of training and testing\n",
    "    Note: it would be more interesting to use a HyperOpt search space:\n",
    "    https://github.com/hyperopt/hyperopt\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_train, X_test):\n",
    "        # Input data\n",
    "        self.train_count = len(X_train)  # 7352 training series\n",
    "        self.test_data_count = len(X_test)  # 2947 testing series\n",
    "        self.n_steps = len(X_train[0])  # 128 time_steps per series\n",
    "\n",
    "        # Training\n",
    "        self.learning_rate = 0.0025\n",
    "        self.lambda_loss_amount = 0.0015\n",
    "        self.training_epochs = 300\n",
    "        self.batch_size = 1500\n",
    "\n",
    "        # LSTM structure\n",
    "        self.n_inputs = len(X_train[0][0])  # Features count is of 9: 3 * 3D sensors features over time\n",
    "        self.n_hidden = 32  # nb of neurons inside the neural network\n",
    "        self.n_classes = 6  # Final output classes\n",
    "        self.W = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_inputs, self.n_hidden])),\n",
    "            'output': tf.Variable(tf.random_normal([self.n_hidden, self.n_classes]))\n",
    "        }\n",
    "        self.biases = {\n",
    "            'hidden': tf.Variable(tf.random_normal([self.n_hidden], mean=1.0)),\n",
    "            'output': tf.Variable(tf.random_normal([self.n_classes]))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Network(_X, config):\n",
    "    \"\"\"Function returns a TensorFlow RNN with two stacked LSTM cells\n",
    "    Two LSTM cells are stacked which adds deepness to the neural network.\n",
    "    Note, some code of this notebook is inspired from an slightly different\n",
    "    RNN architecture used on another dataset, some of the credits goes to\n",
    "    \"aymericdamien\".\n",
    "    Args:\n",
    "        _X:     ndarray feature matrix, shape: [batch_size, time_steps, n_inputs]\n",
    "        config: Config for the neural network.\n",
    "    Returns:\n",
    "        This is a description of what is returned.\n",
    "    Raises:\n",
    "        KeyError: Raises an exception.\n",
    "      Args:\n",
    "        feature_mat: ndarray fature matrix, shape=[batch_size,time_steps,n_inputs]\n",
    "        config: class containing config of network\n",
    "      return:\n",
    "              : matrix  output shape [batch_size,n_classes]\n",
    "    \"\"\"\n",
    "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
    "    # input shape: (batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    # Reshape to prepare input to hidden activation\n",
    "    _X = tf.reshape(_X, [-1, config.n_inputs])\n",
    "    # new shape: (n_steps*batch_size, n_input)\n",
    "\n",
    "    # Linear activation\n",
    "    _X = tf.nn.relu(tf.matmul(_X, config.W['hidden']) + config.biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, config.n_steps, 0)\n",
    "    # new shape: n_steps * (batch_size, n_hidden)\n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(config.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # Get LSTM cell output\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # Get last time step's output feature for a \"many to one\" style classifier,\n",
    "    # as in the image describing RNNs at the top of this page\n",
    "    lstm_last_output = outputs[-1]\n",
    "\n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, config.W['output']) + config.biases['output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset is now located at: UCI HAR Dataset/\n",
      "(12, 9)\n",
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "features shape, labels shape, each features mean, each features standard deviation\n",
      "(2947, 12, 9) (2947, 6) 0.0991508 0.395822\n",
      "the dataset is therefore properly normalised, as expected.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # -----------------------------\n",
    "    # Step 1: load and prepare data\n",
    "    # -----------------------------\n",
    "\n",
    "    # Those are separate normalised input features for the neural network\n",
    "    INPUT_SIGNAL_TYPES = [\n",
    "        \"body_acc_x_\",\n",
    "        \"body_acc_y_\",\n",
    "        \"body_acc_z_\",\n",
    "        \"body_gyro_x_\",\n",
    "        \"body_gyro_y_\",\n",
    "        \"body_gyro_z_\",\n",
    "        \"total_acc_x_\",\n",
    "        \"total_acc_y_\",\n",
    "        \"total_acc_z_\"\n",
    "    ]\n",
    "\n",
    "    # Output classes to learn how to classify\n",
    "    LABELS = [\n",
    "        \"WALKING\",\n",
    "        \"WALKING_UPSTAIRS\",\n",
    "        \"WALKING_DOWNSTAIRS\",\n",
    "        \"SITTING\",\n",
    "        \"STANDING\",\n",
    "        \"LAYING\"\n",
    "    ]\n",
    "\n",
    "    DATA_PATH = \"\"\n",
    "    DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "    print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n",
    "    TRAIN = \"train/\"\n",
    "    TEST = \"test/\"\n",
    "\n",
    "    X_train_signals_paths = [\n",
    "        DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "    ]\n",
    "    X_test_signals_paths = [\n",
    "        DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "    ]\n",
    "    X_train = load_X(X_train_signals_paths)\n",
    "    X_test = load_X(X_test_signals_paths)\n",
    "    #print(X_train[0][120:])\n",
    "    k=0.0\n",
    "    for i in range(128-128//10):\n",
    "        k=k+0.1\n",
    "        X_train=np.delete(X_train,int(k),axis=1)\n",
    "    k=0.0\n",
    "    for i in range(128-128//10):\n",
    "        k=k+0.1\n",
    "        X_test=np.delete(X_test,int(k),axis=1)\n",
    "    #print(k)\n",
    "    #print('h')\n",
    "    print(X_train[0].shape)\n",
    "    \n",
    "    y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "    y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "    y_train = one_hot(load_y(y_train_path))\n",
    "    y_test = one_hot(load_y(y_test_path))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    config = Config(X_train, X_test)\n",
    "    print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "    print(\"features shape, labels shape, each features mean, each features standard deviation\")\n",
    "    print(X_test.shape, y_test.shape,\n",
    "          np.mean(X_test), np.std(X_test))\n",
    "    print(\"the dataset is therefore properly normalised, as expected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    X = tf.placeholder(tf.float32, [None, config.n_steps, config.n_inputs])\n",
    "    Y = tf.placeholder(tf.float32, [None, config.n_classes])\n",
    "\n",
    "    pred_Y = LSTM_Network(X, config)\n",
    "\n",
    "    # Loss,optimizer,evaluation\n",
    "    l2 = config.lambda_loss_amount * \\\n",
    "        sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "    # Softmax loss and L2\n",
    "    cost = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=pred_Y)) + l2\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=config.learning_rate).minimize(cost)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred_Y, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing iter: 0, test accuracy : 0.49202579259872437, loss : 1.8164609670639038\n",
      "traing iter: 1, test accuracy : 0.5052595734596252, loss : 1.6679625511169434\n",
      "traing iter: 2, test accuracy : 0.5724465847015381, loss : 1.5595533847808838\n",
      "traing iter: 3, test accuracy : 0.5792331099510193, loss : 1.5093190670013428\n",
      "traing iter: 4, test accuracy : 0.6189345121383667, loss : 1.398862361907959\n",
      "traing iter: 5, test accuracy : 0.6498133540153503, loss : 1.332991123199463\n",
      "traing iter: 6, test accuracy : 0.6864607930183411, loss : 1.2628469467163086\n",
      "traing iter: 7, test accuracy : 0.6925687193870544, loss : 1.2418748140335083\n",
      "traing iter: 8, test accuracy : 0.7071598172187805, loss : 1.2292225360870361\n",
      "traing iter: 9, test accuracy : 0.7193756103515625, loss : 1.2261228561401367\n",
      "traing iter: 10, test accuracy : 0.6996946334838867, loss : 1.2668321132659912\n",
      "traing iter: 11, test accuracy : 0.7166610360145569, loss : 1.2623350620269775\n",
      "traing iter: 12, test accuracy : 0.7173396944999695, loss : 1.238060712814331\n",
      "traing iter: 13, test accuracy : 0.7234475612640381, loss : 1.2350003719329834\n",
      "traing iter: 14, test accuracy : 0.7237868905067444, loss : 1.2422311305999756\n",
      "traing iter: 15, test accuracy : 0.7343060970306396, loss : 1.2379770278930664\n",
      "traing iter: 16, test accuracy : 0.7329487800598145, loss : 1.2149358987808228\n",
      "traing iter: 17, test accuracy : 0.7417712807655334, loss : 1.1935718059539795\n",
      "traing iter: 18, test accuracy : 0.7434679269790649, loss : 1.2077674865722656\n",
      "traing iter: 19, test accuracy : 0.7468612194061279, loss : 1.2076423168182373\n",
      "traing iter: 20, test accuracy : 0.7482185363769531, loss : 1.1795382499694824\n",
      "traing iter: 21, test accuracy : 0.7539871335029602, loss : 1.1559096574783325\n",
      "traing iter: 22, test accuracy : 0.7550050616264343, loss : 1.160325050354004\n",
      "traing iter: 23, test accuracy : 0.7567017078399658, loss : 1.1689882278442383\n",
      "traing iter: 24, test accuracy : 0.7567017078399658, loss : 1.1582061052322388\n",
      "traing iter: 25, test accuracy : 0.7628096342086792, loss : 1.1257622241973877\n",
      "traing iter: 26, test accuracy : 0.7668815851211548, loss : 1.10951566696167\n",
      "traing iter: 27, test accuracy : 0.7685782313346863, loss : 1.1175212860107422\n",
      "traing iter: 28, test accuracy : 0.7658635973930359, loss : 1.1376349925994873\n",
      "traing iter: 29, test accuracy : 0.7706142067909241, loss : 1.1373717784881592\n",
      "traing iter: 30, test accuracy : 0.7740074396133423, loss : 1.0765024423599243\n",
      "traing iter: 31, test accuracy : 0.7807940244674683, loss : 1.064329981803894\n",
      "traing iter: 32, test accuracy : 0.7763827443122864, loss : 1.0811141729354858\n",
      "traing iter: 33, test accuracy : 0.7719715237617493, loss : 1.1203703880310059\n",
      "traing iter: 34, test accuracy : 0.7767220735549927, loss : 1.0793664455413818\n",
      "traing iter: 35, test accuracy : 0.7889379262924194, loss : 1.042068600654602\n",
      "traing iter: 36, test accuracy : 0.7807940244674683, loss : 1.0408291816711426\n",
      "traing iter: 37, test accuracy : 0.782829999923706, loss : 1.0578399896621704\n",
      "traing iter: 38, test accuracy : 0.786223292350769, loss : 1.0176520347595215\n",
      "traing iter: 39, test accuracy : 0.7899559140205383, loss : 1.0204687118530273\n",
      "traing iter: 40, test accuracy : 0.7899559140205383, loss : 1.0226750373840332\n",
      "traing iter: 41, test accuracy : 0.7957244515419006, loss : 1.0111322402954102\n",
      "traing iter: 42, test accuracy : 0.7980997562408447, loss : 1.0061883926391602\n",
      "traing iter: 43, test accuracy : 0.7994570732116699, loss : 1.0033862590789795\n",
      "traing iter: 44, test accuracy : 0.7994570732116699, loss : 0.9986363649368286\n",
      "traing iter: 45, test accuracy : 0.7997964024543762, loss : 1.013230800628662\n",
      "traing iter: 46, test accuracy : 0.7967424392700195, loss : 1.0089751482009888\n",
      "traing iter: 47, test accuracy : 0.8038683533668518, loss : 0.9893198013305664\n",
      "traing iter: 48, test accuracy : 0.8076009750366211, loss : 0.9946848750114441\n",
      "traing iter: 49, test accuracy : 0.8059043288230896, loss : 0.9869506359100342\n",
      "traing iter: 50, test accuracy : 0.8011537194252014, loss : 1.0036084651947021\n",
      "traing iter: 51, test accuracy : 0.8011537194252014, loss : 1.0232301950454712\n",
      "traing iter: 52, test accuracy : 0.8069223165512085, loss : 0.9840600490570068\n",
      "traing iter: 53, test accuracy : 0.80861896276474, loss : 0.9906421899795532\n",
      "traing iter: 54, test accuracy : 0.8038683533668518, loss : 0.9842385053634644\n",
      "traing iter: 55, test accuracy : 0.8059043288230896, loss : 0.9876909255981445\n",
      "traing iter: 56, test accuracy : 0.8116728663444519, loss : 0.9934701323509216\n",
      "traing iter: 57, test accuracy : 0.8069223165512085, loss : 0.9761985540390015\n",
      "traing iter: 58, test accuracy : 0.8079403042793274, loss : 0.9588688611984253\n",
      "traing iter: 59, test accuracy : 0.8194774389266968, loss : 0.9691122770309448\n",
      "traing iter: 60, test accuracy : 0.80861896276474, loss : 0.9779555201530457\n",
      "traing iter: 61, test accuracy : 0.8154054880142212, loss : 0.9541903734207153\n",
      "traing iter: 62, test accuracy : 0.8262640237808228, loss : 0.9597648978233337\n",
      "traing iter: 63, test accuracy : 0.8177807927131653, loss : 0.967592716217041\n",
      "traing iter: 64, test accuracy : 0.8167628049850464, loss : 0.9524976015090942\n",
      "traing iter: 65, test accuracy : 0.8293179273605347, loss : 0.9442651867866516\n",
      "traing iter: 66, test accuracy : 0.8259246945381165, loss : 0.9548320770263672\n",
      "traing iter: 67, test accuracy : 0.8198167681694031, loss : 0.950812816619873\n",
      "traing iter: 68, test accuracy : 0.829657256603241, loss : 0.9434337615966797\n",
      "traing iter: 69, test accuracy : 0.8272819519042969, loss : 0.9488344192504883\n",
      "traing iter: 70, test accuracy : 0.8276212811470032, loss : 0.9489270448684692\n",
      "traing iter: 71, test accuracy : 0.8289785981178284, loss : 0.9429233074188232\n",
      "traing iter: 72, test accuracy : 0.8323718905448914, loss : 0.9475159645080566\n",
      "traing iter: 73, test accuracy : 0.8340685367584229, loss : 0.9485276341438293\n",
      "traing iter: 74, test accuracy : 0.8340685367584229, loss : 0.9436553716659546\n",
      "traing iter: 75, test accuracy : 0.8344078660011292, loss : 0.9497553110122681\n",
      "traing iter: 76, test accuracy : 0.8340685367584229, loss : 0.9492340087890625\n",
      "traing iter: 77, test accuracy : 0.8367831707000732, loss : 0.9378277063369751\n",
      "traing iter: 78, test accuracy : 0.8340685367584229, loss : 0.9364908933639526\n",
      "traing iter: 79, test accuracy : 0.8327112197875977, loss : 0.9405468702316284\n",
      "traing iter: 80, test accuracy : 0.8367831707000732, loss : 0.9352185130119324\n",
      "traing iter: 81, test accuracy : 0.8357651829719543, loss : 0.9350161552429199\n",
      "traing iter: 82, test accuracy : 0.833050549030304, loss : 0.9422595500946045\n",
      "traing iter: 83, test accuracy : 0.8276212811470032, loss : 0.9664109349250793\n",
      "traing iter: 84, test accuracy : 0.8137088418006897, loss : 1.01235830783844\n",
      "traing iter: 85, test accuracy : 0.8042076826095581, loss : 1.0643866062164307\n",
      "traing iter: 86, test accuracy : 0.8221920728683472, loss : 1.013677954673767\n",
      "traing iter: 87, test accuracy : 0.7967424392700195, loss : 1.058455467224121\n",
      "traing iter: 88, test accuracy : 0.817441463470459, loss : 0.9337755441665649\n",
      "traing iter: 89, test accuracy : 0.8391584753990173, loss : 0.9086072444915771\n",
      "traing iter: 90, test accuracy : 0.8313539028167725, loss : 0.8942092657089233\n",
      "traing iter: 91, test accuracy : 0.8221920728683472, loss : 0.9095057249069214\n",
      "traing iter: 92, test accuracy : 0.8187987804412842, loss : 0.9245986938476562\n",
      "traing iter: 93, test accuracy : 0.817441463470459, loss : 0.9147685170173645\n",
      "traing iter: 94, test accuracy : 0.8405157923698425, loss : 0.9189021587371826\n",
      "traing iter: 95, test accuracy : 0.8289785981178284, loss : 0.8960331678390503\n",
      "traing iter: 96, test accuracy : 0.8374618291854858, loss : 0.8829178810119629\n",
      "traing iter: 97, test accuracy : 0.84764164686203, loss : 0.8981142640113831\n",
      "traing iter: 98, test accuracy : 0.8337292075157166, loss : 0.8936170339584351\n",
      "traing iter: 99, test accuracy : 0.8384798169136047, loss : 0.8989713191986084\n",
      "traing iter: 100, test accuracy : 0.8428910970687866, loss : 0.8963592052459717\n",
      "traing iter: 101, test accuracy : 0.8411944508552551, loss : 0.9005904197692871\n",
      "traing iter: 102, test accuracy : 0.8384798169136047, loss : 0.8995869755744934\n",
      "traing iter: 103, test accuracy : 0.8415337800979614, loss : 0.9010927081108093\n",
      "traing iter: 104, test accuracy : 0.8398371338844299, loss : 0.897912859916687\n",
      "traing iter: 105, test accuracy : 0.8394978046417236, loss : 0.9040151834487915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing iter: 106, test accuracy : 0.8405157923698425, loss : 0.9002662897109985\n",
      "traing iter: 107, test accuracy : 0.8428910970687866, loss : 0.9040108323097229\n",
      "traing iter: 108, test accuracy : 0.8408551216125488, loss : 0.9020636081695557\n",
      "traing iter: 109, test accuracy : 0.8425517678260803, loss : 0.9056077003479004\n",
      "traing iter: 110, test accuracy : 0.842212438583374, loss : 0.9049798250198364\n",
      "traing iter: 111, test accuracy : 0.8425517678260803, loss : 0.9057778120040894\n",
      "traing iter: 112, test accuracy : 0.8432304263114929, loss : 0.9075669646263123\n",
      "traing iter: 113, test accuracy : 0.8425517678260803, loss : 0.906926155090332\n",
      "traing iter: 114, test accuracy : 0.8435697555541992, loss : 0.9081970453262329\n",
      "traing iter: 115, test accuracy : 0.8439090847969055, loss : 0.9095301032066345\n",
      "traing iter: 116, test accuracy : 0.8425517678260803, loss : 0.9096733331680298\n",
      "traing iter: 117, test accuracy : 0.8435697555541992, loss : 0.9104903936386108\n",
      "traing iter: 118, test accuracy : 0.8442484140396118, loss : 0.9121253490447998\n",
      "traing iter: 119, test accuracy : 0.8442484140396118, loss : 0.913029670715332\n",
      "traing iter: 120, test accuracy : 0.8445877432823181, loss : 0.9138712286949158\n",
      "traing iter: 121, test accuracy : 0.8439090847969055, loss : 0.914206862449646\n",
      "traing iter: 122, test accuracy : 0.8432304263114929, loss : 0.9161218404769897\n",
      "traing iter: 123, test accuracy : 0.8445877432823181, loss : 0.9174988269805908\n",
      "traing iter: 124, test accuracy : 0.8445877432823181, loss : 0.9187272787094116\n",
      "traing iter: 125, test accuracy : 0.8432304263114929, loss : 0.920302152633667\n",
      "traing iter: 126, test accuracy : 0.8435697555541992, loss : 0.9209167957305908\n",
      "traing iter: 127, test accuracy : 0.8432304263114929, loss : 0.9204018115997314\n",
      "traing iter: 128, test accuracy : 0.8432304263114929, loss : 0.9214531779289246\n",
      "traing iter: 129, test accuracy : 0.842212438583374, loss : 0.9228965044021606\n",
      "traing iter: 130, test accuracy : 0.8415337800979614, loss : 0.9242433309555054\n",
      "traing iter: 131, test accuracy : 0.8405157923698425, loss : 0.9269362092018127\n",
      "traing iter: 132, test accuracy : 0.8378011584281921, loss : 0.931455135345459\n",
      "traing iter: 133, test accuracy : 0.8357651829719543, loss : 0.937782347202301\n",
      "traing iter: 134, test accuracy : 0.8340685367584229, loss : 0.9478387832641602\n",
      "traing iter: 135, test accuracy : 0.824228048324585, loss : 0.9683277606964111\n",
      "traing iter: 136, test accuracy : 0.8099762201309204, loss : 1.0195759534835815\n",
      "traing iter: 137, test accuracy : 0.7977604269981384, loss : 1.1146609783172607\n",
      "traing iter: 138, test accuracy : 0.8048863410949707, loss : 1.0883291959762573\n",
      "traing iter: 139, test accuracy : 0.8082796335220337, loss : 1.0315452814102173\n",
      "traing iter: 140, test accuracy : 0.7556837201118469, loss : 1.2343202829360962\n",
      "traing iter: 141, test accuracy : 0.7695962190628052, loss : 0.9567994475364685\n",
      "traing iter: 142, test accuracy : 0.7251442074775696, loss : 1.0994179248809814\n",
      "traing iter: 143, test accuracy : 0.7536478042602539, loss : 0.9797170162200928\n",
      "traing iter: 144, test accuracy : 0.824228048324585, loss : 0.8142713308334351\n",
      "traing iter: 145, test accuracy : 0.8327112197875977, loss : 0.8137050867080688\n",
      "traing iter: 146, test accuracy : 0.8201560974121094, loss : 0.8379541635513306\n",
      "traing iter: 147, test accuracy : 0.8133695125579834, loss : 0.8252348899841309\n",
      "traing iter: 148, test accuracy : 0.8282999396324158, loss : 0.8132938146591187\n",
      "traing iter: 149, test accuracy : 0.8418731093406677, loss : 0.8076725006103516\n",
      "traing iter: 150, test accuracy : 0.8418731093406677, loss : 0.8004797697067261\n",
      "traing iter: 151, test accuracy : 0.8466236591339111, loss : 0.8100497126579285\n",
      "traing iter: 152, test accuracy : 0.8445877432823181, loss : 0.8163045644760132\n",
      "traing iter: 153, test accuracy : 0.8415337800979614, loss : 0.8152705430984497\n",
      "traing iter: 154, test accuracy : 0.8415337800979614, loss : 0.8166289329528809\n",
      "traing iter: 155, test accuracy : 0.8435697555541992, loss : 0.8201307058334351\n",
      "traing iter: 156, test accuracy : 0.8401764631271362, loss : 0.8232184648513794\n",
      "traing iter: 157, test accuracy : 0.8401764631271362, loss : 0.8263305425643921\n",
      "traing iter: 158, test accuracy : 0.8462843298912048, loss : 0.8305847644805908\n",
      "traing iter: 159, test accuracy : 0.8452664017677307, loss : 0.8324286937713623\n",
      "traing iter: 160, test accuracy : 0.8435697555541992, loss : 0.8342660665512085\n",
      "traing iter: 161, test accuracy : 0.8462843298912048, loss : 0.8361667394638062\n",
      "traing iter: 162, test accuracy : 0.8459450006484985, loss : 0.8387353420257568\n",
      "traing iter: 163, test accuracy : 0.8456056714057922, loss : 0.8412096500396729\n",
      "traing iter: 164, test accuracy : 0.8456056714057922, loss : 0.8434774875640869\n",
      "traing iter: 165, test accuracy : 0.8456056714057922, loss : 0.8451927900314331\n",
      "traing iter: 166, test accuracy : 0.8456056714057922, loss : 0.8470506072044373\n",
      "traing iter: 167, test accuracy : 0.8466236591339111, loss : 0.848371148109436\n",
      "traing iter: 168, test accuracy : 0.8462843298912048, loss : 0.8496602773666382\n",
      "traing iter: 169, test accuracy : 0.8456056714057922, loss : 0.8509936928749084\n",
      "traing iter: 170, test accuracy : 0.8462843298912048, loss : 0.8520088195800781\n",
      "traing iter: 171, test accuracy : 0.8456056714057922, loss : 0.8530653715133667\n",
      "traing iter: 172, test accuracy : 0.8469629883766174, loss : 0.8542205691337585\n",
      "traing iter: 173, test accuracy : 0.84764164686203, loss : 0.8549681901931763\n",
      "traing iter: 174, test accuracy : 0.84764164686203, loss : 0.8562589883804321\n",
      "traing iter: 175, test accuracy : 0.84764164686203, loss : 0.8567615747451782\n",
      "traing iter: 176, test accuracy : 0.8469629883766174, loss : 0.8583606481552124\n",
      "traing iter: 177, test accuracy : 0.84764164686203, loss : 0.8587621450424194\n",
      "traing iter: 178, test accuracy : 0.8479809761047363, loss : 0.8605014085769653\n",
      "traing iter: 179, test accuracy : 0.8489989638328552, loss : 0.8607364892959595\n",
      "traing iter: 180, test accuracy : 0.8483203053474426, loss : 0.8626507520675659\n",
      "traing iter: 181, test accuracy : 0.8479809761047363, loss : 0.8627170324325562\n",
      "traing iter: 182, test accuracy : 0.8489989638328552, loss : 0.8647037148475647\n",
      "traing iter: 183, test accuracy : 0.8486596345901489, loss : 0.8648895025253296\n",
      "traing iter: 184, test accuracy : 0.8489989638328552, loss : 0.8664942383766174\n",
      "traing iter: 185, test accuracy : 0.84764164686203, loss : 0.8670773506164551\n",
      "traing iter: 186, test accuracy : 0.8479809761047363, loss : 0.8681345582008362\n",
      "traing iter: 187, test accuracy : 0.8486596345901489, loss : 0.8690589666366577\n",
      "traing iter: 188, test accuracy : 0.84764164686203, loss : 0.8699113130569458\n",
      "traing iter: 189, test accuracy : 0.8486596345901489, loss : 0.8706170320510864\n",
      "traing iter: 190, test accuracy : 0.8493382930755615, loss : 0.8720642328262329\n",
      "traing iter: 191, test accuracy : 0.8489989638328552, loss : 0.8718351125717163\n",
      "traing iter: 192, test accuracy : 0.8483203053474426, loss : 0.873916745185852\n",
      "traing iter: 193, test accuracy : 0.8459450006484985, loss : 0.8735915422439575\n",
      "traing iter: 194, test accuracy : 0.8466236591339111, loss : 0.875520646572113\n",
      "traing iter: 195, test accuracy : 0.8459450006484985, loss : 0.8760327696800232\n",
      "traing iter: 196, test accuracy : 0.8466236591339111, loss : 0.8761071562767029\n",
      "traing iter: 197, test accuracy : 0.8445877432823181, loss : 0.8779382109642029\n",
      "traing iter: 198, test accuracy : 0.8435697555541992, loss : 0.8785747289657593\n",
      "traing iter: 199, test accuracy : 0.8428910970687866, loss : 0.879173755645752\n",
      "traing iter: 200, test accuracy : 0.8439090847969055, loss : 0.8808301687240601\n",
      "traing iter: 201, test accuracy : 0.8442484140396118, loss : 0.8790433406829834\n",
      "traing iter: 202, test accuracy : 0.842212438583374, loss : 0.8819055557250977\n",
      "traing iter: 203, test accuracy : 0.8391584753990173, loss : 0.8831292390823364\n",
      "traing iter: 204, test accuracy : 0.8415337800979614, loss : 0.8834540843963623\n",
      "traing iter: 205, test accuracy : 0.8401764631271362, loss : 0.8871524333953857\n",
      "traing iter: 206, test accuracy : 0.8425517678260803, loss : 0.8847957849502563\n",
      "traing iter: 207, test accuracy : 0.8449270725250244, loss : 0.8844917416572571\n",
      "traing iter: 208, test accuracy : 0.8452664017677307, loss : 0.8838021755218506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing iter: 209, test accuracy : 0.8445877432823181, loss : 0.8821864128112793\n",
      "traing iter: 210, test accuracy : 0.8378011584281921, loss : 0.8869985342025757\n",
      "traing iter: 211, test accuracy : 0.8337292075157166, loss : 0.8955273628234863\n",
      "traing iter: 212, test accuracy : 0.8350865244865417, loss : 0.9034322500228882\n",
      "traing iter: 213, test accuracy : 0.8313539028167725, loss : 0.9163557887077332\n",
      "traing iter: 214, test accuracy : 0.8249067068099976, loss : 0.9650537371635437\n",
      "traing iter: 215, test accuracy : 0.826603353023529, loss : 0.9385390281677246\n",
      "traing iter: 216, test accuracy : 0.8147268295288086, loss : 1.0467275381088257\n",
      "traing iter: 217, test accuracy : 0.8279606103897095, loss : 0.9661310911178589\n",
      "traing iter: 218, test accuracy : 0.8184594511985779, loss : 0.9789538979530334\n",
      "traing iter: 219, test accuracy : 0.8347471952438354, loss : 0.8275090456008911\n",
      "traing iter: 220, test accuracy : 0.8367831707000732, loss : 0.7841627597808838\n",
      "traing iter: 221, test accuracy : 0.838819146156311, loss : 0.822432279586792\n",
      "traing iter: 222, test accuracy : 0.8391584753990173, loss : 0.7993491888046265\n",
      "traing iter: 223, test accuracy : 0.8442484140396118, loss : 0.7882602214813232\n",
      "traing iter: 224, test accuracy : 0.8432304263114929, loss : 0.7562170624732971\n",
      "traing iter: 225, test accuracy : 0.8401764631271362, loss : 0.7597910165786743\n",
      "traing iter: 226, test accuracy : 0.8459450006484985, loss : 0.7670572400093079\n",
      "traing iter: 227, test accuracy : 0.8401764631271362, loss : 0.7550199031829834\n",
      "traing iter: 228, test accuracy : 0.8449270725250244, loss : 0.786670446395874\n",
      "traing iter: 229, test accuracy : 0.8456056714057922, loss : 0.7678627967834473\n",
      "traing iter: 230, test accuracy : 0.8493382930755615, loss : 0.7835624814033508\n",
      "traing iter: 231, test accuracy : 0.8456056714057922, loss : 0.7686532735824585\n",
      "traing iter: 232, test accuracy : 0.8415337800979614, loss : 0.7745980024337769\n",
      "traing iter: 233, test accuracy : 0.8473023176193237, loss : 0.7720081210136414\n",
      "traing iter: 234, test accuracy : 0.8439090847969055, loss : 0.771248459815979\n",
      "traing iter: 235, test accuracy : 0.8405157923698425, loss : 0.7912957668304443\n",
      "traing iter: 236, test accuracy : 0.8289785981178284, loss : 0.8208063840866089\n",
      "traing iter: 237, test accuracy : 0.8059043288230896, loss : 0.9119873046875\n",
      "traing iter: 238, test accuracy : 0.798439085483551, loss : 0.9050889611244202\n",
      "traing iter: 239, test accuracy : 0.8306752443313599, loss : 0.7980972528457642\n",
      "traing iter: 240, test accuracy : 0.8262640237808228, loss : 0.7879410982131958\n",
      "traing iter: 241, test accuracy : 0.8303359150886536, loss : 0.8117435574531555\n",
      "traing iter: 242, test accuracy : 0.8428910970687866, loss : 0.7897319793701172\n",
      "traing iter: 243, test accuracy : 0.8398371338844299, loss : 0.7898849248886108\n",
      "traing iter: 244, test accuracy : 0.8473023176193237, loss : 0.7988101243972778\n",
      "traing iter: 245, test accuracy : 0.842212438583374, loss : 0.7979137301445007\n",
      "traing iter: 246, test accuracy : 0.8384798169136047, loss : 0.7997995615005493\n",
      "traing iter: 247, test accuracy : 0.8405157923698425, loss : 0.7945747971534729\n",
      "traing iter: 248, test accuracy : 0.8415337800979614, loss : 0.8065418601036072\n",
      "traing iter: 249, test accuracy : 0.8394978046417236, loss : 0.7950097918510437\n",
      "traing iter: 250, test accuracy : 0.8439090847969055, loss : 0.798469066619873\n",
      "traing iter: 251, test accuracy : 0.8418731093406677, loss : 0.8050014972686768\n",
      "traing iter: 252, test accuracy : 0.8439090847969055, loss : 0.8047070503234863\n",
      "traing iter: 253, test accuracy : 0.8445877432823181, loss : 0.8072922229766846\n",
      "traing iter: 254, test accuracy : 0.8442484140396118, loss : 0.8156871795654297\n",
      "traing iter: 255, test accuracy : 0.8462843298912048, loss : 0.8174427151679993\n",
      "traing iter: 256, test accuracy : 0.8456056714057922, loss : 0.8216913938522339\n",
      "traing iter: 257, test accuracy : 0.8432304263114929, loss : 0.8311076164245605\n",
      "traing iter: 258, test accuracy : 0.8452664017677307, loss : 0.8293066024780273\n",
      "traing iter: 259, test accuracy : 0.8418731093406677, loss : 0.8374501466751099\n",
      "traing iter: 260, test accuracy : 0.8442484140396118, loss : 0.8385931253433228\n",
      "traing iter: 261, test accuracy : 0.8435697555541992, loss : 0.8394497632980347\n",
      "traing iter: 262, test accuracy : 0.8428910970687866, loss : 0.8472349643707275\n",
      "traing iter: 263, test accuracy : 0.8425517678260803, loss : 0.8459092378616333\n",
      "traing iter: 264, test accuracy : 0.8391584753990173, loss : 0.8517608642578125\n",
      "traing iter: 265, test accuracy : 0.8428910970687866, loss : 0.8547660708427429\n",
      "traing iter: 266, test accuracy : 0.8408551216125488, loss : 0.8556373119354248\n",
      "traing iter: 267, test accuracy : 0.8398371338844299, loss : 0.8630819916725159\n",
      "traing iter: 268, test accuracy : 0.8401764631271362, loss : 0.8575214743614197\n",
      "traing iter: 269, test accuracy : 0.8428910970687866, loss : 0.8614698052406311\n",
      "traing iter: 270, test accuracy : 0.8381404876708984, loss : 0.8656558990478516\n",
      "traing iter: 271, test accuracy : 0.8347471952438354, loss : 0.8677658438682556\n",
      "traing iter: 272, test accuracy : 0.8367831707000732, loss : 0.8854857087135315\n",
      "traing iter: 273, test accuracy : 0.8340685367584229, loss : 0.8682101964950562\n",
      "traing iter: 274, test accuracy : 0.8344078660011292, loss : 0.8947778940200806\n",
      "traing iter: 275, test accuracy : 0.8198167681694031, loss : 0.899811863899231\n",
      "traing iter: 276, test accuracy : 0.8340685367584229, loss : 0.8843222856521606\n",
      "traing iter: 277, test accuracy : 0.8449270725250244, loss : 0.831784725189209\n",
      "traing iter: 278, test accuracy : 0.8310145735740662, loss : 0.8321993947029114\n",
      "traing iter: 279, test accuracy : 0.842212438583374, loss : 0.8261152505874634\n",
      "traing iter: 280, test accuracy : 0.8276212811470032, loss : 0.8271241784095764\n",
      "traing iter: 281, test accuracy : 0.84764164686203, loss : 0.8475441932678223\n",
      "traing iter: 282, test accuracy : 0.8459450006484985, loss : 0.8259438872337341\n",
      "traing iter: 283, test accuracy : 0.8500169515609741, loss : 0.8530736565589905\n",
      "traing iter: 284, test accuracy : 0.8415337800979614, loss : 0.8254103064537048\n",
      "traing iter: 285, test accuracy : 0.8259246945381165, loss : 0.8626241683959961\n",
      "traing iter: 286, test accuracy : 0.8306752443313599, loss : 0.870323657989502\n",
      "traing iter: 287, test accuracy : 0.8398371338844299, loss : 0.8599540591239929\n",
      "traing iter: 288, test accuracy : 0.820834755897522, loss : 0.896573543548584\n",
      "traing iter: 289, test accuracy : 0.8411944508552551, loss : 0.789095401763916\n",
      "traing iter: 290, test accuracy : 0.8462843298912048, loss : 0.7633355855941772\n",
      "traing iter: 291, test accuracy : 0.8551068902015686, loss : 0.7604545950889587\n",
      "traing iter: 292, test accuracy : 0.84764164686203, loss : 0.7216796875\n",
      "traing iter: 293, test accuracy : 0.8442484140396118, loss : 0.7663708925247192\n",
      "traing iter: 294, test accuracy : 0.8527315855026245, loss : 0.7357715368270874\n",
      "traing iter: 295, test accuracy : 0.8469629883766174, loss : 0.7996352314949036\n",
      "traing iter: 296, test accuracy : 0.8527315855026245, loss : 0.7401618361473083\n",
      "traing iter: 297, test accuracy : 0.8462843298912048, loss : 0.7680565118789673\n",
      "traing iter: 298, test accuracy : 0.8408551216125488, loss : 0.7514066696166992\n",
      "traing iter: 299, test accuracy : 0.8350865244865417, loss : 0.7883175611495972\n",
      "\n",
      "final test accuracy: 0.8350865244865417\n",
      "best epoch's test accuracy: 0.8551068902015686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=False))\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    y_pre=y_test[0]\n",
    "    best_accuracy = 0.0\n",
    "    # Start training for each batch and loop epochs\n",
    "    for i in range(config.training_epochs):\n",
    "        for start, end in zip(range(0, config.train_count, config.batch_size),\n",
    "                              range(config.batch_size, config.train_count + 1, config.batch_size)):\n",
    "            sess.run(optimizer, feed_dict={X: X_train[start:end],\n",
    "                                           Y: y_train[start:end]})\n",
    "\n",
    "        # Test completely at every epoch: calculate accuracy\n",
    "        pred_out, accuracy_out, loss_out = sess.run(\n",
    "            [pred_Y, accuracy, cost],\n",
    "            feed_dict={\n",
    "                X: X_test,\n",
    "                Y: y_test\n",
    "            }\n",
    "        )\n",
    "        print(\"traing iter: {},\".format(i) +\n",
    "              \" test accuracy : {},\".format(accuracy_out) +\n",
    "              \" loss : {}\".format(loss_out))\n",
    "        if accuracy_out>best_accuracy:\n",
    "            y_pre=pred_out\n",
    "        best_accuracy = max(best_accuracy, accuracy_out)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"final test accuracy: {}\".format(accuracy_out))\n",
    "    print(\"best epoch's test accuracy: {}\".format(best_accuracy))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]]\n",
      "Col 0\n",
      "[[2382   69]\n",
      " [  64  432]]\n",
      "\n",
      "Col 1\n",
      "[[2385   91]\n",
      " [  83  388]]\n",
      "\n",
      "Col 2\n",
      "[[2434   93]\n",
      " [  48  372]]\n",
      "\n",
      "Col 3\n",
      "[[2394   62]\n",
      " [  75  416]]\n",
      "\n",
      "Col 4\n",
      "[[2351   64]\n",
      " [  69  463]]\n",
      "\n",
      "Col 5\n",
      "[[2410    0]\n",
      " [  40  497]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = np.zeros_like(y_pre)\n",
    "b[np.arange(len(y_pre)), y_pre.argmax(1)] = 1\n",
    "y_pre=b\n",
    "print(y_pre)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "for i in range(y_test.shape[1]):\n",
    "    print(\"Col {}\".format(i))\n",
    "    print(confusion_matrix(y_test[:,i], y_pre[:,i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: Tensor(\"mul_7:0\", shape=(), dtype=float32)%\n",
      "\n",
      "Precision: 87.44197667611375%\n",
      "Recall: 87.13946386155412%\n",
      "f1_score: 87.22918966256603%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[432  18  46   0   0   0]\n",
      " [ 36 388  47   0   0   0]\n",
      " [ 30  18 372   0   0   0]\n",
      " [  0  11   0 416  64   0]\n",
      " [  3   4   0  62 463   0]\n",
      " [  0  40   0   0   0 497]]\n",
      "\n",
      "Confusion matrix (normalised to % of total test data):\n",
      "[[ 14.6589756    0.61079061   1.56090939   0.           0.           0.        ]\n",
      " [  1.22158122  13.1659317    1.5948422    0.           0.           0.        ]\n",
      " [  1.01798439   0.61079061  12.62300587   0.           0.           0.        ]\n",
      " [  0.           0.37326095   0.          14.11605072   2.1717       0.        ]\n",
      " [  0.10179844   0.13573125   0.           2.10383439  15.71089172   0.        ]\n",
      " [  0.           1.35731244   0.           0.           0.          16.86460876]]\n",
      "Note: training and testing data is not equally distributed amongst classes, \n",
      "so it is normal that more than a 6th of the data is correctly classifier in the last category.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzUAAAM+CAYAAAAjKy8vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzs3Xm0LVV57/3v7xxARFppVFA4ahAR\nacQjIkkUmySYoGKir2ITMBgSc9UENHYwFI2KIWKLxouK2IJNJFfRBEhyEUVADtIjNghII8IB6aQ/\n+3n/qNrXxWY36zR7rz3d388Ya7CqatacT9XG4XqYT81KVSFJkiRJrVo06gAkSZIkaXWY1EiSJElq\nmkmNJEmSpKaZ1EiSJElqmkmNJEmSpKaZ1EiSJElqmkmNJEmSpKaZ1EiSJElqmkmNJEmSpKaZ1EiS\nJElq2lqjDkCSJEnSqtsrqeWjDmIK58BJVbXXbI9jUiNJkiQ1bDmwbNRBTCGw2VyMY/mZJEmSpKY5\nUyNJkiS1bvE8natYMTYnw8zTq5ckSZKk4ZjUSJIkSWqa5WeSJElSywIszqijmNyKuRnGmRpJkiRJ\nTTOpkSRJktQ0y88kSZKkpmX+rn42R/Vn8/XqJUmSJGkoJjWSJEmSmmb5mSRJktSyAGvN09XP5ogz\nNZIkSZKaZlIjSZIkqWmWn0mSJEktC/N49bO5sbCvXpIkSVLzTGokSZIkNc3yM0mSJKl1i139TJIk\nSZKaZVIjSZIkqWmWn0mSJEktS1z9bNQBSJIkSdLqMKmRJEmS1DTLzyRJkqSW+fJNZ2okSZIktc2k\nRpIkSVLTLD+TJEmSWufLNyVJkiSpXSY1kiRJkppm+ZkkSZLUMl++6UyNJEmSpLaZ1EiSJElqmuVn\nkiRJUutc/UySJEmS2mVSI0mSJGkkkhyT5PokF03Y/7okP05ycZIjZurH8jNJkiSpZQHWanau4ljg\nKOBz4zuSPBN4AbBTVd2dZIuZOmn26iVJkiS1rapOA26asPs1wPuq6u6+zfUz9WNSI0mSJGk+eRzw\nh0nOSvKdJE+Z6QTLzyRJkqSWJfN59bPNkiwb2D66qo6e4Zy1gE2A3YGnAF9J8piqqulOkCRJkqTZ\nsLyqlq7kOVcDX++TmB8kGQM2A26Y6gTLzyRJkiTNJ/8OPAsgyeOAdYDl053gTI0kSZLUusVtzlUk\nOQ7Yk65M7WrgHcAxwDH9Ms/3APtNV3oGJjWSJEmSRqSq9p3i0CtWpp82UzpJkiRJ6jlTI0mSJLUs\nNFt+tqYs7KuXJEmS1DyTGkmSJElNs/xMkiRJatn8fvnmnHCmRpIkSVLTTGokSZIkNc3yM0mSJKl1\nrn4mSZIkSe0yqZEkSZLUNMvPJEmSpJYFVz8bdQCSJEmStDpMaiRJkiQ1zfIzSZIkqWlx9bNRByBJ\nkiRJq8OkRpIkSVLTLD+TJEmSWubqZ87USJIkSWqbSY0kSZKkpll+JkmSJLUsuPrZqAOQJEmSpNVh\nUiNJkiSpaZafSZIkSa1z9TNJkiRJapdJjSRpjUny4CTfTHJLkq+uRj8vT3LymoxtVJL8YZIfjzoO\nSfpdZvmZJC1ASV4GHAw8HrgNOA94T1V9bzW7fhHwMGDTqrpvVTupqi8CX1zNWGZdkgK2raqfTdWm\nqr4LbDd3UUlacBJXPxt1AJKkuZXkYOBDwHvpEpCtgY8DL1gD3W8D/GR1EprfJUn8j4eSNAdMaiRp\nAUmyEfAu4H9V1der6jdVdW9VfbOq/rFv86AkH0pybf/5UJIH9cf2THJ1kjckuT7JL5O8qj/2TuDt\nwEuS3J7kgCSHJfnCwPhLktT4j/0k+yf5eZLbklye5OUD+783cN4eSc7uy9rOTrLHwLFTk/xTktP7\nfk5OstkU1z8e/5sG4t8nyZ8m+UmSm5K8baD9bknOSHJz3/aoJOv0x07rm53fX+9LBvp/c5LrgM+M\n7+vPeWw/xq799pZJlifZc7X+sJK0wJnUSNLC8jRgXeCEadocAuwO7ALsDOwGHDpw/OHARsBWwAHA\nx5JsUlXvoJv9+XJVrV9Vn54ukCQPAT4CPLeqNgD2oCuDm9juocC3+rabAh8AvpVk04FmLwNeBWwB\nrAO8cZqhH053D7aiS8I+CbwCeDLwh8Dbkzymb7sCOAjYjO7ePRv4O4CqenrfZuf+er880P9D6Wat\nDhwcuKouA94MfDHJesBngGOr6tRp4pWkmS1eND8/c8SkRpIWlk2B5TOUh70ceFdVXV9VNwDvBF45\ncPze/vi9VfVt4HZW/ZmRMeCJSR5cVb+sqosnafNnwE+r6vNVdV9VHQdcCjxvoM1nquonVXUn8BW6\nhGwq99I9P3QvcDxdwvLhqrqtH/9iYCeAqjqnqs7sx70C+N/AM4a4pndU1d19PPdTVZ8EfgqcBTyC\nLomUJK0GkxpJWlhuBDab4VmPLYErB7av7Pf9vz4mJEV3AOuvbCBV9RvgJcDfAr9M8q0kjx8invGY\nthrYvm4l4rmxqlb038eTjl8NHL9z/Pwkj0tyYpLrktxKNxM1aWnbgBuq6q4Z2nwSeCLw0aq6e4a2\nkqQZmNRI0sJyBnAXsM80ba6lK50at3W/b1X8BlhvYPvhgwer6qSq+iO6GYtL6X7szxTPeEzXrGJM\nK+Nf6eLatqo2BN4GzPSGu5ruYJL16RZq+DRwWF9eJ0mrLnQv35yPnzliUiNJC0hV3UL3HMnH+gfk\n10uydpLnJjmib3YccGiSzfsH7t8OfGGqPmdwHvD0JFv3ixS8dfxAkocleX7/bM3ddGVsKybp49vA\n45K8LMlaSV4CPAE4cRVjWhkbALcCt/ezSK+ZcPxXwGMecNb0PgycU1WvpntW6BOrHaUkLXAmNZK0\nwFTVB+jeUXMocANwFfBa4N/7Ju8GlgEXABcCP+z3rcpYpwBf7vs6h/snIouAN9DNxNxE96zK303S\nx43A3n3bG4E3AXtX1fJViWklvZFuEYLb6GaRvjzh+GHAZ/vV0f6/mTpL8gJgL7qSO+j+DruOr/om\nSVo1qZp2llySJEnSPLZ0kwfXsj0fPeowJpV//9E5VbV0tsdxpkaSJElS00xqJEmSJDVtuiU9JUmS\nJLVgDlcam4+cqZEkSZLUNJMaSZpBksOT/MOo45hJkiVJavzFmkn+I8l+a3iM/ZN8b032ORf65aNP\nS3JbkiNHMP68vW9JTk3y6lnq+wNJ/nbmlpK0ekxqJGkaSTYH/hL436OOZWVV1XOr6rNzNd7EpGoV\nzn9UkjOT3DQx8Ujyn0lWZ/WcA4HlwIZV9YZJxj42ydDLVq9s+xn6Wq37NltxTdH/FUmesxKn/Atw\nSJJ1ZismSfQv31w0Pz9zxKRGkqa3P/DtqrpzTXe8Jn7E/o55K/BZ4NHAPuNJTP+yzZ9X1bLV6Hsb\n4JLyPQZzqqp+CVwKPH/UsUj63WZSI0nTey7wnfGNJHsmuTrJG5Jcn+SXSV41cHyjJJ9LckOSK5Mc\nmmRRf2z/JKcn+WCSm4DDJuy7OcnPk+zR77+qH2O/gf7/LMm5SW7tjx82VeCDZUVJfi/Jd5LckmR5\nki8PtHt8klP6GZIfD75EMsmmSb7Rj/cD4LHT3KvT+n/enOT2JE9Lsqi/B1f21/K5JBtNcf6jgf+p\nqluAs4HHJNkQeAvwtmnGHY91jyRn99d4dpI9+v3HAvsBb+rjes6E8w4EXj5w/Jv9/u37e3hzkouT\nPH+G9m9Jcllf4nZJkhfOFPNU963v76+S/CjJr5OclGSbfn/6f1+u76/1giRPnCquSe7THyW5tD/3\nKLr/xjt+7LFJ/ifJjf2/J19MsnF/7PPA1sA3+/7f1O//apLr+v5OS7LDhCFPBf5syHshSavEpEaS\nprcj8OMJ+x4ObARsBRwAfCzJJv2xj/bHHgM8g6507VUD5z4V+DmwBfCegX0XAJsCXwKOB54C/B7w\nCuCoJOv3bX/T97kx3Q/F1yTZZ4jr+CfgZGAT4JF9nCR5CHBKP+4WwL7Axwd+mH4MuAt4BPBX/Wcq\nT+//uXFVrV9VZ9DNdO0PPLO/J+sDR01x/kXAH/U/opcCl/Rxf6iqbp7u4pI8FPgW8BG6+/gB4FtJ\nNq2q/YEvAkf0cf3X4LlVdfSE489LsjbwTbp7tgXwOuCLSbabrH3f1WXAH9L9/d8JfCHJI6aLu/eA\n+9b/Td8G/DmwOfBd4Li+3R/35zyO7t+DlwA3ThPX4H3aDPg34FBgsz7m3x9sAhwObAlsDzwKOKy/\nT68EfgE8r+//iP6c/wC27e/TD/sYBv0I2HmI+yBpVSXd6mfz8TNHTGokaXobA7dN2Hcv8K6qureq\nvg3cDmyXZDHdD8y3VtVtVXUFcCTwyoFzr62qj1bVfQMlbZdX1WeqagXwZbofku+qqrur6mTgHroE\nh6o6taourKqxqrqA7ofuM4a4jnvpSrC2rKq7qmr8ofW9gSv68e+rqh/S/eh9UX89fwG8vap+U1UX\n0ZWHrYyXAx+oqp9X1e10JWYvzeSld4fTJQXfoUum1gZ2opsZ+FI/C/DaKcb5M+CnVfX5/jqOoyt7\nesAP+yHtTpeAva+q7qmq/wFOpEv6JlVVX62qa/u/zZeBnwK7reL4fwMcXlU/qqr7gPcCu/SzNfcC\nGwCPB9K3+eWQ/f4pXRne16rqXuBDwHUD1/Czqjql/3fvBrrkcNp/v6rqmP7f97vpEqCdJ8zG3Ub3\nvyNJmjUmNZI0vV/T/YAcdGP/Q3PcHXQ/gDcD1gGuHDh2Jd2MzrirJhnjVwPf7wSoqon71gdI8tQk\n/zddedstwN/2487kTXT/Ff4HfSnV+IzLNsBT+xKrm5PcTJeIPJxuhmCtCTEPXtswtuSB92Mt4GET\nG1bVTVX1kqraGfgw3WzS6+jKzy4CngP8bZInDDHO+FhbTdJ22LivqqqxYftL8pdJzhu4j09kuL/N\nZLYBPjzQ1010f7+t+gTrKLrE71dJju7L9IaxJQN/z/4Zo/+3nWSLJMcnuSbJrcAXpruGJIuTvK8v\nu7sVuKI/NHjOBsC0M22StLpMaiRpehfQlfkMYzm/nREZtzVwzcD26j6o/iXgG8Cjqmoj4BMMPBMx\nlaq6rqr+uqq2pJsF+HiS36P7Qfudqtp44LN+Vb0GuAG4j27maPB6phxmkn3X8sD7cR/3T+QmcyBw\nZj87tCOwrKruAS6kSxZmGmd8rGsmaTuZibFfCzwq/fNQk/R3v/b9DMongdcCm1bVxnSJ2DC1F5Pd\nt6uAv5nwd3lwVX0foKo+UlVPBnag+/fzH6fpa9AvGfh7Jgn3//se3vexU1VtSFf+OHgNE/t/GfAC\nuoRzI2DJeNcDbbYHzp8hLkmra9SrnLn6mSTNa99muPIu+vKxrwDvSbJB/0P3YLr/2r2mbADcVFV3\nJdmN7kfljJK8OMkj+81f0/04XUFXUvW4JK9Msnb/eUqS7fvr+Trdggbr9TMk07335gZgjO7ZmXHH\nAQcleXT/XNB7gS9PmOmaGOsWwP+if5YDuBx4Zn/+Urpnkib6dn8dL0uyVroV057QX98wfjUh7rPo\nnl96U39P9qQrZTt+ivYPobunN/TX8ComT74mM9l9+wTw1vFnm9ItQPHi/vtT+hm7tfsY76L7W04W\n10TfAnZI8ud9CeDr6Wblxm1AV055c5Kt+G2yNG5i/xsAdwM3AuvR/X0negbdczeSNGtMaiRpep8D\n/jTJg4ds/zq6H5o/B75HN7NyzBqM5++AdyW5DXg7XRI1jKcAZyW5nW6m5++r6vKquo3uwfOX0s1O\nXAf8M/Cg/rzX0pW+XQccC3xmqgGq6g66xQ9O78umdqe79s/TrfB1Od0P8NfNEOv76Z4pur3fPhx4\nFt3sxTcmW9q5qm6kez7oDXQ/sN8E7F1Vy2cYa9yngSf0cf97Pyv0fLrV75YDHwf+sqounaL9JXTP\nT51B98N/R+D0YQae7L5V1Ql0f4fj+7Kui/pYADakmxX6NV1J3I109+wBcU0y1nLgxcD7+vO2nRDn\nO4FdgVvoEqCvT+jicODQvv830v3v40q6GaxLgDMHG/cLJTwBeEAskrQmxSX7JWl6Sd4LXF9VHxp1\nLFJL0r1E9bKq+vioY5F+ly3dbL1atvd2ow5jUvnseedU1eq8PHkovvhNkmZQVTO+I0XSA1XVG0Yd\ng6SFwfIzSZIkSU1zpkaSJElqWuZ0pbH5aGFfvSRJkqTmmdRIkiRJaprlZ1qjNlt3rVqywYNmbqg1\n7qpbth91CAvaPcMu+Kw17sG3jjoCSQvNzVzBHbV8mJfrzo0Ai+dPOKNgUqM1askGD2LZPv64HoXX\n/+fZow5hQbt6h7FRh7Bg7XTS4lGHIGmBOZpZX6FYK8nyM0mSJElNc6ZGkiRJallw9bNRByBJkiRJ\nq8OkRpIkSVLTLD+TJEmSmubLNxf21UuSJElqnkmNJEmSpKZZfiZJkiS1LMCihf3yTWdqJEmSJDXN\npEaSJElS0yw/kyRJklrn6meSJEmS1C6TGkmSJElNs/xMkiRJalmAxa5+JkmSJEnNMqmRJEmS1DTL\nzyRJkqSmxdXPRh2AJEmSJK0OkxpJkiRJTbP8TJIkSWqZq585UyNJkiSpbSY1kiRJkppm+ZkkSZLU\nukULe65iYV+9JEmSpOaZ1EiSJElqmuVnkiRJUssSVz8bdQCSJEmSFqYkxyS5PslFkxx7Y5JKstlM\n/ZjUSJIkSRqVY4G9Ju5M8ijgj4BfDNOJ5WeSJElSywIsbnOuoqpOS7JkkkMfBN4E/J9h+jGpkSRJ\nkjRbNkuybGD76Ko6eroTkjwfuKaqzk+Ge1bIpEaSJEnSbFleVUuHbZxkPeAQ4I9XZhCTGkmSJKl1\nvzurnz0WeDQwPkvzSOCHSXarquumOsmkRpIkSdK8UFUXAluMbye5AlhaVcunO6/NJ4okSZIkNS/J\nccAZwHZJrk5ywKr040yNJEmS1LIEFrU5V1FV+85wfMkw/bR59ZIkSZLUM6mRJEmS1DTLzyRJkqTW\n/e6sfrZKnKmRJEmS1DSTGkmSJElNs/xMkiRJalmAxQt7rmJhX/2IJflgkn8Y2D4pyacGto9McnD/\n/aAkdyXZaOD4nklOnKTfU5Ms7b8vSfLTJH8y2D7J/knGkuw0cN5FSZb039dP8q9JLktybpJzkvz1\nmr8LkiRJ0uoxqRmt7wN7ACRZBGwG7DBwfA/g9P77vsDZwAuH7TzJI4GTgDdU1UmTNLkaOGSK0z8F\n/BrYtqqeBOwFPHTYsSVJkqS5YlIzWqfTJzV0ycxFwG1JNknyIGB74NwkjwXWBw6lS26G8XDgZODQ\nqvrGFG1OBHZIst3gzn683fpzxwCq6oaq+ufhL02SJElzZlHm52euLn/ORtIDVNW1wH1JtqZLbs4A\nzgKeBiwFLqiqe+gSmeOA7wLbJdliiO4/BxxVVV+dps0YcATwtgn7dwDOH09oJEmSpPnMpGb0xmdr\nxpOaMwa2v9+3eSlwfJ9kfB148RD9/hfwyiTrzdDuS8DuSR49VYMkhyQ5L8m1Uxw/MMmyJMtuuOu+\nIUKTJEmS1hyTmtEbf65mR7ryszPpZmr2AE7vH+TfFjglyRV0Cc4wJWhH0M36fDXJlKvcVdV9wJHA\nmwd2XwLs3D/nQ1W9p6p2ATacoo+jq2ppVS3dfF0X1JMkSZpTSbf62Xz8zBGTmtE7HdgbuKmqVlTV\nTcDGdInNGXQJzGFVtaT/bAlslWSbIfo+CLgV+HSS6YoajwWeA2wOUFU/A5YB706yGCDJunQLBkqS\nJEnziknN6F1It+rZmRP23VJVy+lmZk6YcM4J/X6AZye5euDztPFGVVXAfsAj6GZuJtU/t/MRYPBZ\nnVcDmwI/S3IOXTnbmyc5XZIkSRopa4VGrKpWMKGsq6r2H/j+gGddqurggc0HT9LtngNt7wH+eODY\nqf3+Y+lmaMbbfYQusRnfvhX4myEuQZIkSaM2hyuNzUfO1EiSJElqmkmNJEmSpKaZ1EiSJElqms/U\nSJIkSS0Lc7p88ny0sK9ekiRJUvNMaiRJkiQ1zfIzSZIkqWlxSedRByBJkiRJq8OkRpIkSVLTLD+T\nJEmSWubqZ87USJIkSWqbSY0kSZKkpll+JkmSJLXO1c8kSZIkqV0mNZIkSZKaZvmZJEmS1LLE1c9G\nHYAkSZIkrQ6TGkmSJElNs/xMkiRJap2rn0mSJElSu0xqJEmSJDXN8jNJkiSpZcHVz0YdgCRJkiSt\nDpMaSZIkSU2z/EySJElqWlz9bNQBSJIkSdLqMKmRJEmS1DTLzyRJkqSWBVi0sOcqFvbVS5IkSWqe\nSY0kSZKkpll+JkmSJLVusaufSZIkSVKzTGokSZIkNc3yM0mSJKlliaufjToASZIkSVodJjWSJEmS\nmmb5mSRJktS6Ra5+JkmSJEnNMqmRJEmS1DTLzyRJkqSWBV++OeoAJEmSJGl1mNRIkiRJaprlZ1qj\nLr9ze15x0Q9GHcaC9IX3vnTUISxo+xz/pVGHIElayHz5piRJkiS1y6RGkiRJUtMsP5MkSZJaljDm\nyzclSZIkqV0mNZIkSZKaZvmZJEmS1LACxlz9TJIkSZLaZVIjSZIkqWmWn0mSJEmNc/UzSZIkSWqY\nSY0kSZKkpll+JkmSJDWsElYsXthzFQv76iVJkiQ1z6RGkiRJUtMsP5MkSZIa5+pnkiRJktQwkxpJ\nkiRJTbP8TJIkSWpZoBYt7LmKhX31kiRJkppnUiNJkiSpaZafSZIkSQ0rXP3MmRpJkiRJTTOpkSRJ\nktQ0y88kSZKkliWWn406AEmSJEkLU5Jjklyf5KKBff+S5NIkFyQ5IcnGM/VjUiNJkiRpVI4F9pqw\n7xTgiVW1E/AT4K0zdWL5mSRJktSwbvWzNucqquq0JEsm7Dt5YPNM4EUz9dPm1UuSJElaCP4K+I+Z\nGjlTI0mSJGm2bJZk2cD20VV19DAnJjkEuA/44kxtTWokSZKkxs3j1c+WV9XSlT0pyX7A3sCzq6pm\nam9SI0mSJGneSLIX8GbgGVV1xzDn+EyNJEmSpJFIchxwBrBdkquTHAAcBWwAnJLkvCSfmKkfZ2ok\nSZKkhlXCirQ5V1FV+06y+9Mr20+bVy9JkiRJPZMaSZIkSU2z/EySJElq3Dxe/WxOOFMjSZIkqWkm\nNZIkSZKaZvmZJEmS1DjLzyRJkiSpYSY1kiRJkppm+ZkkSZLUsArUooU9V7Gwr16SJElS80xqJEmS\nJDXN8jNJkiSpaXH1s1EHMIwkH0zyDwPbJyX51MD2kUkO7r8flOSuJBsNHN8zyYmT9HtqkqX99yVJ\nfprkTwbbJ9k/yViSnQbOuyjJkv77+kn+NcllSc5Nck6Sv57mWh4QS5Jjk7xoIKYfJzk/yelJtuv3\n7933f36SS5L8TZJDkpzXf1YMfH/9QN/nJzluyPHOTrLLQLu/SnJhkgv6a37BVNclSZIkjUoTSQ3w\nfWAPgCSLgM2AHQaO7wGc3n/fFzgbeOGwnSd5JHAS8IaqOmmSJlcDh0xx+qeAXwPbVtWTgL2Ahw47\n9hReXlU7A58F/iXJ2sDRwPP6/U8CTq2q91TVLlW1C3Dn+Peq+kh/XdvT/Y2fnuQhQ4z3ceBf+nMf\n2V/zH1TVTsDuwAWreV2SJEnSGtdKUnM6fVJDl8xcBNyWZJMkDwK2B85N8lhgfeBQuuRmGA8HTgYO\nrapvTNHmRGCH8VmTcf14u/XnjgFU1Q1V9c/DX9q0TgN+D9iArlTwxn6Mu6vqx0Oc/zLg83TX9/wh\n2p8BbNV/3wK4Dbi9H/P2qrp8paKXJEnS7AuMLVo0Lz9zpYmkpqquBe5LsjVdcnMGcBbwNGApcEFV\n3UOXyBwHfBfYLskWQ3T/OeCoqvrqNG3GgCOAt03YvwNw/nhCMwueB1xYVTcB3wCuTHJckpf3M1Yz\neQnwZbp7MkyStxfw7/3384FfAZcn+UyS5011UpIDkyxLsuyue28YYhhJkiRpzWkiqemNz9aMJzVn\nDGx/v2/zUuD4Psn4OvDiIfr9L+CVSdabod2XgN2TPHqqBgPPuFw7TT81xP4vJjkP+H3gjQBV9Wrg\n2cAP+n3HTBdskqcAN1TVlcB/A7sm2WSK5l9McjXwZuCj/Xgr6JKcFwE/AT6Y5LBJA686uqqWVtXS\nddfefLqwJEmSpDWupaRm/LmaHenKz86km6nZAzi9f5B/W+CUJFfQJTjDzE4cQTfr89UkU64GV1X3\nAUfS/fAfdwmw8/isyfgzLsCG04x3IzAxuXgosHxg++X9szH7VNVVAzFcWFUfBP4I+IsZrmtf4PH9\nvbisj2mqc14OPJoucfvYwHhVVT+oqsPp7udMY0qSJGmOFTCWzMvPXGkpqTkd2Bu4qapW9CVZG9Ml\nNmfQ/Yg/rKqW9J8tga2SbDNE3wcBtwKfTqa9+8cCzwE2B6iqnwHLgHcnWQyQZF1guj5+CmzZP8RP\nH9/OwHlTndCvsLbnwK5dgCunab+IbpZqp/H7AbyAaZK8qrqX7lmk3ZNsn2TLJLsOO6YkSZI0Ki0l\nNRfSrXp25oR9t1TVcrqZhBMmnHNCvx/g2UmuHvg8bbxRVRWwH/AIupmbSfXP7XyE7iH6ca8GNgV+\nluQcunK2N09y+ngfdwOvAD7Tl5h9DXh1Vd0y5ZV3SdKb+qWXzwPeCew/TfunA9dU1TUD+04DnpDk\nEdPEdifdbNQbgbWB9ye5tB/zJcDfTzOmJEmSNBLNvHyzf8Zjwwn79h/4/oBnXarq4IHNB0/S7Z4D\nbe8B/njg2Kn9/mPpZmjG232ELrEZ374V+JshLmEwrtPplkie7Niek+y7DfjTGfpcf+D7qRP77+/f\neEKz/1TjVdWRA5vPmm5MSZIkzQ++fFOSJEmSGtbMTE1rkuxI946YQXdX1VNHEY8kSZL0u8qkZpZU\n1YV0D9dLkiRJs6aSOX3R5Xy0sK9ekiRJUvNMaiRJkiQ1zfIzSZIkqXEr5vBFl/ORMzWSJEmSmmZS\nI0mSJKlplp9JkiRJDStw9bNRByBJkiRJq8OkRpIkSVLTLD+TJEmSmhbK1c8kSZIkqV0mNZIkSZKa\nZvmZJEmS1LLA2CLLzyRJkiSpWSY1kiRJkppm+ZkkSZLUsALGsrDnKhb21UuSJElqnkmNJEmSpKZZ\nfiZJkiQ1ztXPJEmSJKlhJjUV2ytWAAAgAElEQVSSJEmSmmb5mSRJktSyhLFYfiZJkiRJzTKpkSRJ\nktQ0y88kSZKkhhWwYtHCnqtY2FcvSZIkqXkmNZIkSZKaZvmZJEmS1DhXP5MkSZKkhpnUSJIkSWqa\n5WeSJElSwwrLz5ypkSRJktQ0kxpJkiRJTbP8TJIkSWpZQvnyTUmSJElql0mNJEmSpKZZfiZJkiQ1\nztXPJEmSJKlhJjWSJEmSmmb5mdaojME6d4w6ioXp9W87ftQhLGgnfOKAUYewYL3z28eMOgRJGilf\nvulMjSRJkqTGmdRIkiRJaprlZ5IkSVLjLD+TJEmSpIaZ1EiSJElqmuVnkiRJUsMqYSwLe65iYV+9\nJEmSpOaZ1EiSJElqmuVnkiRJUuNc/UySJEmSGmZSI0mSJKlplp9JkiRJDStgxSLLzyRJkiSpWSY1\nkiRJkppm+ZkkSZLUMl++6UyNJEmSpLaZ1EiSJElqmuVnkiRJUuPKl29KkiRJUrtMaiRJkiSNRJJj\nklyf5KKBfQ9NckqSn/b/3GSmfkxqJEmSpIYVMEbm5WcIxwJ7Tdj3FuC/q2pb4L/77WmZ1EiSJEka\niao6Dbhpwu4XAJ/tv38W2GemfkxqJEmSJM0nD6uqXwL0/9xiphNc/UySJElq3Nj8Xf1ssyTLBraP\nrqqj1/QgJjWSJEmSZsvyqlq6kuf8KskjquqXSR4BXD/TCZafSZIkSZpPvgHs13/fD/g/M53gTI0k\nSZLUtDCWNucqkhwH7ElXpnY18A7gfcBXkhwA/AJ48Uz9mNRIkiRJGomq2neKQ89emX7aTOkkSZIk\nqedMjSRJktSwYl6vfjYnnKmRJEmS1DSTGkmSJElNs/xMkiRJallgheVnkiRJktQukxpJkiRJTbP8\nTJIkSWqYq585UyNJkiSpcSY1kiRJkppm+ZkkSZLUtDC2wOcqFvbVS5IkSWqeSY0kSZKkpll+JkmS\nJDWuXP1MkiRJktplUiNJkiSpabOW1CT5YJJ/GNg+KcmnBraPTHJw//2gJHcl2Wjg+J5JTpyk31OT\nLO2/L0ny0yR/Mtg+yf5JxpLsNHDeRUmW9N/XT/KvSS5Lcm6Sc5L89TTXsiTJnX3bHyX5QZL9JrTZ\nJ8kFSS5NcmGSffr9Oyc5b6DdvknuSLJ2v71jkgsGrm3ZQNulSU7tv6+X5It93xcl+V6SbZKc13+u\nS3LNwPY6/XkvTFJJHj/hei4auM+39Nd2aZL3D7R7WJITk5yf5JIk357qHkmSJGk0xl++OR8/c2U2\nZ2q+D+wBkGQRsBmww8DxPYDT++/7AmcDLxy28ySPBE4C3lBVJ03S5GrgkClO/xTwa2DbqnoSsBfw\n0BmGvKyqnlRV2wMvBQ5K8qo+lp2B9wMvqKrHA88H3t8nVRcC2yTZoO9nD+BS4EkD26cPjLNFkudO\nMv7fA7+qqh2r6onAAcB1VbVLVe0CfAL44Ph2Vd3Tn7cv8L0+5ql8t78PTwL2TvL7/f53AadU1c5V\n9QTgLTPcI0mSJGnOzWZSczp9UkOXzFwE3JZkkyQPArYHzk3yWGB94FC6H+DDeDhwMnBoVX1jijYn\nAjsk2W5wZz/ebv25YwBVdUNV/fOwF1ZVPwcOBl7f73oj8N6qurw/fjlwOPCP/RhnA0/t2z4Z+Bi/\nvTd70CWA4/6F7l5M9AjgmoEYflxVd08XZ5L1gd+nS4CmS2rG+7wTOA/YamDMqweOXzBTH5IkSdJc\nm7WkpqquBe5LsjXdD/czgLOApwFLgQv62YR9geOA7wLbJdliiO4/BxxVVV+dps0YcATwtgn7dwDO\nH09oVsMPgfGSrh2AcyYcX8ZvZ6a+D+yR5CF9XKdy/6RmcKbmDODuJM+c0N8xwJuTnJHk3Um2HSLG\nfYD/rKqfADcl2XW6xkk2AbYFTut3fQz4dJL/m+SQJFtOcd6BSZYlWXbXfTcMEZYkSZLWpDEyLz9z\nZbYXChifrRlPas4Y2B6fnXgpcHyfZHwdePEQ/f4X8Mok683Q7kvA7kkePVWD/sf6eUmuHWLc+506\n4XtNcnx83/h92A04u6ouA34vyebA+v3Mz6B3M2G2pqrOAx5DN5PzUODsJNvPEOO+wPH99+OZeibs\nD/vneq4DTqyq6/oxT+rH/CRdAnduH/P9VNXRVbW0qpauu9YDDkuSJEmzaraTmvHnanakKz87k26m\nZg/g9P6Zk22BU5JcQZfgDFOCdgTdrM9Xk0z5rp2qug84EnjzwO5LgJ3753yoqvf0z6RsuHKXxpOA\nH/XfL6abfRq0az8WdNf9FOAP6BI76Mq6Xsr9S8/G4/4fYF1g9wn7b6+qr1fV3wFfAP50quCSbAo8\nC/hUf2//EXhJMukTW9+tqp3o/k6vSbLLwJg3VdWXquqVdGV0T59qTEmSJGkU5mKmZm/gpqpaUVU3\nARvTJTZn0CUwh1XVkv6zJbBVkm2G6Psg4Fa68qjp5raOBZ4DbA5QVT+jKw17d5LFAEnWheHnx/pV\n1N4PfLTf9X7grQOrqy2hK3s7sh/zNuAqYH9+m9ScAfwDkyQ1vfcAbxoY8/f78jD6lc2eAFw5TZgv\nAj5XVdv09/ZRwOV0idWk+jK1w+mTwCTPGp8N6xc6eCzwi2nGlCRJ0hwrwlgWzcvPXJntkS6kW/Xs\nzAn7bqmq5XQzFSdMOOcEfvtQ+7OTXD3wedp4o6oqYD+6h9mPmCqA/rmdjwCDz+q8GtgU+FmSc+jK\n2d48yemDHju+pDPwFeCjVfWZfozz+vO/meRS4JvAm/r9404HHlRVV/XbZ9CVdk2a1FTVt4HBB1Qe\nC3wnyYXAuXSJ2b9NE+++PPDe/hvwshmu8xPA0/uSvScDy/rStDOAT1XV2TOcL0mSJM2pdLmBtGZs\ntt7Sev62Pxh1GAvS+jfN3cN4eqAPf+KAUYewYL1z72NGHYKkBeZolnJtLZs3/8e75MmPqbef9U+j\nDmNSB6z9inOqauJjGmvclM+jSJIkSWrDXK40Nh+Z1AxIsiPw+Qm7766qp07WXpIkSdLomdQMqKoL\ngV1mbChJkiRp3jCpkSRJkhpWgbFpFwP+3Td366xJkiRJ0iwwqZEkSZLUNMvPJEmSpMatWOCrnzlT\nI0mSJKlpJjWSJEmSmmb5mSRJktSwIq5+NuoAJEmSJGl1mNRIkiRJaprlZ5IkSVLjytXPJEmSJKld\nJjWSJEmSmmb5mSRJktS4sSzsuYqFffWSJEmSmmdSI0mSJKlpU5afJdlwuhOr6tY1H44kSZKklVHA\n2AJf/Wy6Z2ouprtHg3dofLuArWcxLkmSJEkaypRJTVU9ai4DkSRJkqRVMdTqZ0leCjymqt6b5JHA\nw6rqnNkNTZIkSdLMsuDLz2ZcKCDJUcAzgVf2u+4APjGbQUmSJEnSsIaZqdmjqnZNci5AVd2UZJ1Z\njkuSJEmShjJMUnNvkkV0iwOQZFNgbFajkiRJkjQ0y89m9jHg34DNk7wT+B7wz7MalSRJkiQNacaZ\nmqr6XJJzgOf0u15cVRfNbliSJEmSNJyhVj8DFgP30pWgDTO7I0mSJGkOFLAilp9NK8khwHHAlsAj\ngS8leetsByZJkiRJwxhmpuYVwJOr6g6AJO8BzgEOn83AJEmSJGkYwyQ1V05otxbw89kJR5IkSdLK\nWuirn02Z1CT5IF2J3h3AxUlO6rf/mG4FNEmSJEkauelmasZXOLsY+NbA/jNnLxxJkiRJWjlTJjVV\n9em5DESSJEnSyivC2AJfoHjGZ2qSPBZ4D/AEYN3x/VX1uFmMS5IkSZKGMkxKdyzwGSDAc4GvAMfP\nYkySJEmSNLRhkpr1quokgKq6rKoOBZ45u2FJkiRJGlaRefmZK8Ms6Xx3kgCXJflb4Bpgi9kNS5Ik\nSZKGM0xScxCwPvB6umdrNgL+ajaDkiRJkqRhzZjUVNVZ/dfbgFfObjiSJEmSVpYv35xCkhPoXrY5\nqar681mJSJIkSZJWwnQzNUfNWRT6nbHOnbD1BQt7nfRRuWOjKf8bhObAO/c+ZtQhLFiHHbLnqENY\n0Pa89pRRh7Bg7fmZtUcdgjRvTPfyzf+ey0AkSZIkrbzC8jP/k7okSZKkppnUSJIkSWraMEs6A5Dk\nQVV192wGI0mSJGnlWX42gyS7JbkQ+Gm/vXOSj856ZJIkSZI0hGHKzz4C7A3cCFBV5wPPnM2gJEmS\nJGlYw5SfLaqqK5P7TWmtmKV4JEmSJK2EIqxY4OVnwyQ1VyXZDagki4HXAT+Z3bAkSZIkaTjDlJ+9\nBjgY2Br4FbB7v0+SJEmSRm7GmZqquh546RzEIkmSJGkVlOVn00vySboXld5PVR04KxFJkiRJ0koY\n5pma/xr4vi7wQuCq2QlHkiRJklbOMOVnXx7cTvJ54JRZi0iSJEnSSvHlmyvv0cA2azoQSZIkSVoV\nwzxT82t++0zNIuAm4C2zGZQkSZIkDWvapCbdGzd3Bq7pd41V1QMWDZAkSZI0GgWsKMvPptQnMCdU\n1Yr+Y0IjSZIkaY1IclCSi5NclOS4JOuuSj/DPFPzgyS7rkrnkiRJkjSZJFsBrweWVtUTgcWs4vsx\npyw/S7JWVd0H/AHw10kuA34DhG4Sx0RHkiRJmgcaXv1sLeDBSe4F1gOuXdVOpvIDYFdgn1XpWJIk\nSdKCt1mSZQPbR1fV0QBVdU2S9wO/AO4ETq6qk1dlkOmSmvSDXbYqHUuSJEla8JZX1dLJDiTZBHgB\n3Stjbga+muQVVfWFlR1kuqRm8yQHT3Wwqj6wsoNJkiRJWrOKUG2Wnz0HuLyqbgBI8nVgD2CNJjWL\ngfWhzTskSZIkaV77BbB7kvXoys+eDSyb/pTJTZfU/LKq3rUqnUqSJEnSdKrqrCRfA34I3AecCxy9\nKn3N+EyNJEmSpPltbKg3tcw/VfUO4B2r2890V//s1e1ckiRJkmbblElNVd00l4FIkiRJ0qqYrvxM\nkiRJUgPGamE/OdJm8Z0kSZIk9UxqJEmSJDXN8jNJkiSpYQWsWOALFztTI0mSJKlpJjWSJEmSmmb5\nmSRJktS0UK5+JkmSJEntMqmRJEmS1DTLzyRJkqSGFTDm6meSJEmS1C6TGkmSJElNs/xMkiRJalnB\nClc/kyRJkqR2mdRIkiRJaprlZ5IkSVLDXP3MmRpJkiRJjTOpmceSHJLk4iQXJDkvyVOTnJpkaZKz\n+n2/SHJD//28JL+aYv+SJFck2azvu5IcOTDWG5McNrD9in7ci5Ocn+RTSTYewW2QJEmSpmX52TyV\n5GnA3sCuVXV3n4ysM368qp7at9sfWFpVr51w/gP2J/eblrwb+PMkh1fV8gnn7gUcBDy3qq5JshjY\nD3gYcPMau0hJkiStEeXqZ5qnHgEsr6q7AapqeVVduwb7vw84mi55megQ4I1VdU0/9oqqOqaqfrwG\nx5ckSZLWCJOa+etk4FFJfpLk40meMQtjfAx4eZKNJuzfAfjhsJ0kOTDJsiTL7uCGNRqgJEmSNBOT\nmnmqqm4HngwcCNwAfLkvKVuTY9wKfA54/VRtkuzYP5NzWZKXTNHP0VW1tKqWrsfmazJESZIkzSiM\nzdPPXDGpmcf6sq9Tq+odwGuBv5iFYT4EHAA8ZGDfxcCufQwXVtUuwH8AD56F8SVJkqTVYlIzTyXZ\nLsm2A7t2Aa5c0+NU1U3AV+gSm3GHA+9P8siBfSY0kiRJmpdc/Wz+Wh/4aL+M8n3Az+hK0b42C2Md\nSTcTBEBVfTvJ5sB/9Cuf3QxcBJw0C2NLkiRpNRQwtsBXPzOpmaeq6hxgj0kO7Tmh3bHAsZOc/4D9\nVbVk4Pv6A99/Baw3oe1ngc+uXNSSJEnS3LP8TJIkSVLTnKmRJEmSGrdigZefOVMjSZIkqWkmNZIk\nSZKaZvmZJEmS1LiawxddzkfO1EiSJElqmkmNJEmSpKZZfiZJkiQ1zJdvOlMjSZIkqXEmNZIkSZKa\nZvmZJEmS1LKKL98cdQCSJEmStDpMaiRJkiQ1zfIzSZIkqWHd6mejjmK0nKmRJEmS1DSTGkmSJElN\ns/xMkiRJaly5+pkkSZIktcukRpIkSVLTLD+TJEmSGtatfmb5mSRJkiQ1y6RGkiRJUtMsP5MkSZIa\nN4blZ5IkSZLULJMaSZIkSU2z/EySJElqWAErXP1MkiRJktplUiNJkiSpaZafSZIkSS2rUJafSZIk\nSVK7TGokSZIkNc3yM0mSJKlxY2OWn0mSJElSs0xqJEmSJDXN8jNJkiSpYb5805kaSZIkSY0zqZEk\nSZLUNMvPJEmSpJYVjFl+JkmSJEntMqmRJEmS1DTLzyRJkqTGleVnkiRJktQuZ2q0xo0trlGHsCCt\nc+eoI5BG41lXnzLqEBa0Uy9ZMuoQFqzDuGbUIUjzhjM1kiRJkprmTI0kSZLUsCIu6TzqACRJkiRp\ndZjUSJIkSWqa5WeSJElS48YW+DpNztRIkiRJappJjSRJkqSmWX4mSZIkNawKVoy5+pkkSZIkNcuk\nRpIkSVLTLD+TJEmSGle+fFOSJEmSRiPJxkm+luTSJD9K8rSV7cOZGkmSJEmj9GHgP6vqRUnWAdZb\n2Q5MaiRJkqTGjTVafpZkQ+DpwP4AVXUPcM/K9mP5mSRJkqRReQxwA/CZJOcm+VSSh6xsJyY1kiRJ\nkmbLZkmWDXwOnHB8LWBX4F+r6knAb4C3rOwglp9JkiRJDSvm9cs3l1fV0mmOXw1cXVVn9dtfYxWS\nGmdqJEmSJI1EVV0HXJVku37Xs4FLVrYfZ2okSZIkjdLrgC/2K5/9HHjVynZgUiNJkiS1rNLs6mcA\nVXUeMF2J2owsP5MkSdL/396dh9tVlncf//5OlEkJVCaRwQjixKBgGIS3gEKtrYh1KsbWikattopo\nwXmqFScQay21DWrR2qJSxBelCiqDIoMGyigKDqAgvIATgxMk9/vH2oHt4SSQc87eK+us7+e69sVe\nz9pn799ZCUnu89zreaROs6iRJEmS1Gm2n0mSJEkdVkAtbztFu5ypkSRJktRpFjWSJEmSOs32M0mS\nJKnjurz62WxwpkaSJElSp1nUSJIkSeo0288kSZKkLitYvtz2M0mSJEnqLIsaSZIkSZ1m+5kkSZLU\nYQUsc/UzSZIkSeouixpJkiRJnWb7mSRJktRx5epnkiRJktRdFjWSJEmSOs32M0mSJKnDClhebado\nlzM1kiRJkjrNokaSJElSp9l+JkmSJHVZhWWufiZJkiRJ3WVRI0mSJKnTLGrGJMmbklye5JIkFyU5\nY/Df7yX55eD5RUn2HLx+kyR3JPnrSe9zdZITh46fneS4wfODk9yU5H+TXJXk1BXvNzh/XJJnD56f\nmWTp0LmFSc4cOt5t8JqrklyY5JQkO47q+kiSJGl6Cli+PGvkY1y8p2YMkjwBOADYpap+m2RjYK2q\n+kmSfYHDquqASV/2HOA8YBHwb5POLUyyfVVdPsXHfbqqXjH43CcCn03yxKq6YorXbprkT6rqi5Py\nbgZ8BnheVZ0zGPs/wLbApavxrUuSJEkj50zNeGwO3FxVvwWoqpur6if38jWLgL8DtkyyxaRzRwFv\nvLcPraozgCXAS1fykiOBN08x/grg4ysKmsF7nV1Vn7u3z5QkSZLGzaJmPE4DtkpyZZJ/SbLPql6c\nZCvgwVX1TZoZk4MmveQzwC5JHn4fPvtC4FErOXcu8NvBjM6w7QdfJ0mSpA6oyhr5GBeLmjGoqtuA\nx9PMmNwEfDrJwav4kufSFC4An6KZtRm2jGaW5Q334ePv7XfTO5l6tubuN0jOT3JFkg+u5PxLkyxN\nsvRX3HQfIkmSJEmzx6JmTKpqWVWdWVVvo2nvetYqXr4IODjJ1cDJwGOTbDfpNf8B7A1sfS8fvTMw\n1f00K3KdDqwD7DE0fDmwy9BrdgfeAmywkvdYUlULq2rhemxyL3EkSZKk2WVRMwZJHjmpKHkccM3K\nXgs8oKq2qKoFVbUAeDfN7M1dquoO4APAoav43H1oZoeOvZeIRwCvHTo+hqao2nNobL17eQ9JkiS1\noWD58jXzMS6ufjYeDwQ+lGRD4E7ge6z85v1FwEmTxk6kaUP7h0njH+WerWMHDVYqWw/4IfCslax8\ndpeq+p8kNw0d35DkIOC9g0UKbgRuBt6xqveRJEmS2mBRMwZVdQGw50rOnQmcOXT89ilecwnwmMHz\nBUPjvwUeMnR8HHDcKnIcPPR830nnHj/p+DxglQsaSJIkSWsCixpJkiSpw1Zsvtln3lMjSZIkqdMs\naiRJkiR1mu1nkiRJUpcVLLP9TJIkSZK6y6JGkiRJUqfZfiZJkiR1WBFXP2s7gCRJkiTNhEWNJEmS\npE6z/UySJEnquFredoJ2OVMjSZIkqdMsaiRJkiR1mu1nkiRJUpcVLCtXP5MkSZKkzrKokSRJktRp\ntp9JkiRJHVbg5pttB5AkSZKkmbCokSRJktRptp9JkiRJHbfczTclSZIkqbssaiRJkiR1mu1nkiRJ\nUpcVlKufSZIkSVJ3WdRIkiRJ6jTbzyRJkqQOc/NNZ2okSZIkdZxFjSRJkqROs/1MkiRJ6rKCZW6+\nKUmSJEndZVEjSZIkqdNsP5MkSZI6rIirn7UdQJIkSZJmwqJGkiRJUqfZfiZJkiR1WUEts/1MkiRJ\nkjrLokaSJElSp9l+JkmSJHVY4eabztRIkiRJ6jSLGkmSJEmdZvuZJEmS1HFuvilJkiRJHWZRI0mS\nJKnTbD/TrJvo+eZPbblyr54ve9KyR33N3/dt2fvj9287Qq+9nevajtBbb8c/d9ryhbYDTFawvOf/\nDHCmRpIkSVKnWdRIkiRJ6jTbzyRJkqSOyxq6+lmN6XOcqZEkSZLUaRY1kiRJkjrN9jNJkiSpywrm\nraGrz945ps9xpkaSJElSp1nUSJIkSWpNknlJ/jfJtLcAsv1MkiRJ6rAAE93efPNVwBXA/Om+gTM1\nkiRJklqRZEvgqcBHZvI+FjWSJEmS2vKPwGuBGc012X4mSZIkdVmFiTV0801g4yRLh46XVNUSgCQH\nADdW1QVJ9p3Jh1jUSJIkSRqVm6tq4UrO7QUcmORPgXWA+Uk+WVV/ubofYvuZJEmSpLGrqjdU1ZZV\ntQB4LnD6dAoacKZGkiRJ6rwsaztBuyxqJEmSJLWqqs4Ezpzu19t+JkmSJKnTnKmRJEmSOiwF89bc\n1c/GwpkaSZIkSZ1mUSNJkiSp02w/kyRJkjpuYnnbCdrlTI0kSZKkTrOokSRJktRptp9JkiRJHZaC\niWWufiZJkiRJnWVRI0mSJKnTbD+TJEmSOi5uvilJkiRJ3WVRI0mSJKnTbD+TJEmSOiwF85a1naJd\nztRIkiRJ6jSLGkmSJEmdZvuZJEmS1GlhwtXPJEmSJKm7LGokSZIkdZrtZ5IkSVKXFUy4+pkkSZIk\ndZdFjSRJkqROs/1MkiRJ6rAAcfUzSZIkSeouixpJkiRJnWb7mSRJktRlBfNc/UySJEmSusuipiOS\n3LaKcxcnOX7o+KVJPj10PD/J95M8LMlxSZ49GD8zydKh1y1McubQ8W6D11yV5MIkpyTZcda/OUmS\nJGkGLGo6LsmjaX4d907ygMHwscCWSfYfHL8D+FhV/XCKt9g0yZ9M8b6bAZ8B3lhV21XVLsC7gW1n\n/ZuQJEnStAWYWL5mPsbFoqb7ngf8B3AacCBAVRXwcuAfkywE9gOOXMnXHwm8eYrxVwAfr6pzVgxU\n1dlV9blZzC5JkiTNmEVN9x0EfBo4Hli0YrCqLgFOBb4KHFJVv1vJ158L/DbJEyeNbw9cOPtxJUmS\npNllUdNhSXYFbqqqa2iKl12S/MHQS44BrquqM+7lrd7J1LM1w591fpIrknxwinMvTbI0ydJfcdNq\nfheSJEmakYKJZVkjH+NiUdNti4BHJbka+D4wH3jW0Pnlg8cqVdXpwDrAHkPDlwO7DL1md+AtwAZT\nfP2SqlpYVQvXY5NpfBuSJEnS9FnUdFSSCeA5wE5VtaCqFgBPZ6gFbTUdAbx26PgY4OAkew6NrTfN\n95YkSZJGxs03u2O9JNcOHR9N01p23dDY14DHJNm8qq5fnTevqv9JctPQ8Q1JDgLem2QL4EbgZpqV\n1CRJkrQGyRhXGlsTWdR0RFVNNat29KTXLAM2Hzq+Gthh0msOHnq+76Rzj590fB6wzzQjS5IkSWNh\n+5kkSZKkTnOmRpIkSeqwFMwb40pjayJnaiRJkiR1mkWNJEmSpE6z/UySJEnquIllbSdolzM1kiRJ\nkjrNokaSJElSp9l+JkmSJHVYCiaWu/qZJEmSJHWWRY0kSZKkTrP9TJIkSeq4uPqZJEmSJHWXRY0k\nSZKkTrP9TJIkSeqyCvOWufqZJEmSJHWWRY0kSZKkTrP9TJIkSeqwFEy4+pkkSZIkdZdFjSRJkqRO\ns/1MkiRJ6riJ5W0naJczNZIkSZI6zaJGkiRJUqfZfiZJkiR1WUHcfFOSJEmSusuiRpIkSVKn2X4m\nSZIkdViAeW6+KUmSJEndZVEjSZIkqdNsP5MkSZK6rGDC9jNJkiRJ6i6LGkmSJEmdZvuZJEmS1GEB\nJtx8U5IkSZK6y6JGkiRJUqfZfiZJkiR1WUGWtx2iXc7USJIkSeo0ixpJkiRJnWb7mSRJktRhAea5\n+aYkSZIkdZdFjSRJkqROs/1MkiRJ6rJy801naiRJkiR1mkWNJEmSpE6z/Uyz6nouuPnvyTVt55iB\njYGb2w4xLV9rO8CMdffazw1e//Z47dvT6Wv/920HmLkuX/+Hth3g9xRM9Hz1M4sazaqq2qTtDDOR\nZGlVLWw7Rx957dvl9W+P1749Xvt2ef01m2w/kyRJktRpztRIkiRJHRZsP3OmRvp9S9oO0GNe+3Z5\n/dvjtW+P175dXn+RZKskZyS5IsnlSV41rfepqtnOJkmSJGlM5m/8+Nr1gPPajjGl0z++1gWruncq\nyebA5lV1YZL1gQuAP6uqb6/O59h+JkmSJHVZhzffrKrrgesHz29NcgWwBbBaRY3tZ5IkSZJal2QB\nsDNw/up+rTM1kiRJkogd5IAAABz8SURBVEZl4yRLh46XVNU97qdK8kDgRODQqrpldT/EokbS2CV5\nGnBJVV0zOH4r8CzgGuBVVfXDNvP1SZKNgL2BH1XVBW3nmeuSzAc2q6qrBsfPAdYdnD61qv5fa+Ek\nddYavvrZzfe2H1GS+9MUNP9ZVZ+dzofYfqZeSrI4yeFDx9cluSXJrUle3ma2njgCuAkgyQHAXwIv\nAk4G/rXFXHNeki8k2WHwfHPgMppr/x9JDm01XD8cBew1dPxuYFeawnIObBC/5kqyfZIDh44/kORj\ng8cubWbrA6+/ViZJgI8CV1TV0dN9H4sa9dXLgI8NHd9YVfOBTYBF7UTqlaqqXw2ePxP4aFVdUFUf\nofk10Og8rKouGzx/IfDlqnoasDtNcaPR2hX4+NDxrVX1yqp6MbBDS5n64j3AzUPHfwycApwBvLWV\nRP3i9dfK7AU8H3hSkosGjz9d3Tex/Ux9NVFVPx06PgGgqn6TZN2VfI1mTwa9s78C9gP+ZejcOu1E\n6o07hp7vBxwLd604s7ydSL1yv/r9vRSeP/R8w3GH6ZnNq+qcoeNbqupEgCR/3VKmPvH6j1Kt0e1n\nq1RVZ9N00M2IRY36aoPhg6p6F0CSCWCjVhL1yz8CFwG30Ew3LwVIsjODZR01Mj9O8krgWmAX4EsA\ng2L+/m0G64nlSR5cVTcArJg1S7IFYFE5WusPH1TVHkOHm445Sx95/TVStp+pr05L8s4pxt8BnDbu\nMH1TVR8D9gEWA8NTzDcAB7eRqUcWA9vTXOeDquoXg/E9gH9vK1SPHAl8PsneSdYfPPYBPjc4p9H5\nSZLdJw8m2QP4SQt5+sbrr5FypkZ9dTjwkSTfAy4ejD0WWAq8uLVUPVJV1wHXTRqeDxwGvGT8ifqh\nqm6kuads8vgZSX7QQqReqapPJrkZeCdNcQnNYg1vraovtpesF14HfDrJccCFg7HHAy8ADmorVI94\n/Uepw+1ns8WiRr1UVbcDi5Jsw93/sPh2VX2/xVi9kWQnmlWgHkLzE+oP0dxXszvw/haj9UKSJ9Ds\n1vy1qrpx8OvxeuAPga1aDdcDVfUlBm1/Gp+q+uZgVuBvuXtG+HJgD5fSHj2vv0bNoka9lGTrwdM7\nuXum5q7xqvpRG7l65Fjgw8C5wFNofmr3X8BfVNVv2gw21yU5EjiA5p6m1yX5AvA3wLtw9bORG+zJ\ntDJVVf8wtjA9NPjHsytttcTrr1GyqFFfnQIUv7/aRtEsJ7wpMK+NUD2ydlUdN3j+3SSHAa+vqp5P\nno/FU4GdByv9/QFNL/tOKzaD1MjdPsXYA2juddoIsKgZkSRn0Pw5P5Wqqv3GmadvvP6jFcLEshkv\nINZpFjXqparacfg4yQKaft/9aX5irdFaZ7DS2Yo/gW8DdhpswEVVXbjSr9RM/XrFbFhV/TzJdy1o\nxqeq7mqvTLI+8Cqa/YI+ha2Xo3bYFGN7AK8Fbhxzlj7y+mukLGrUa0m2A97E3fdyHFJVd6z6qzQL\nbgCOXslxAU8ae6L+2DbJyUPHC4aPq+rAKb5GsyjJg4DXAH9BsxHnLlX183ZTzX1VdcGK54MV594C\nrA28zEUaRs/rr1GzqFEvJdmBppjZHngfsNjWp/Gpqn3bztBjT5907OzAGA3uaXomsATYsapuazlS\nryT5Y5p/TP8GOKKqzmg5Uq94/UfI1c/I729sLPVDkmXAj2nurbnHHwNVdcjYQ/VIkmeu6nxVfXZc\nWaRxSrIc+C3NIiXDfwGH5r6C+a0E64Ek36K5b/JImkVKfo9tr6Pl9R+tDTdcWPvsfX7bMaZ08ufv\nd0FVLRz15zhTo75azMpvWNToPW0V5wqwqBmRJJcy9e/9Ff+o3mnMkXqlqtz0uj2309y/9+zBY5ht\nr6Pn9ddIWdSol4ZW3lILquqFKzuXZLNxZumhA9oO0GeD+2lWqqp+Nq4sfWPba7u8/qMV288satRP\nST7PKmZqvFl6vJJsADwLeB7waJqNITUCVXXNVONJ9qK5/n873kS9cwH3XE5+hQK2GW+c/rDttV1e\nf42aRY366qi2A/RdknWBA2n+Ib0LsD7wZ8DX2szVJ0keR3P9/xz4Ibb9jcO+KyssNXK2vbbL66+R\nsqhRX61VVV+e6kSS9wJnjTlPryT5T2Bv4DTgn4HTge9V1Zlt5uqDJI8AngssAn4KfJpm0Zgnthqs\nP06iKeI1Zqtqe9VYvN2CfrT63n7mDYvqq2OSPHV4IMlEkuOAx7YTqVd2AH4OXAF8Z7Cctgs3jMd3\ngP2Ap1XV/6mqDzHFCoAamX5v+d2yJI9M8v4kpwweRw0KfY3eV5O8Pok/UNdI+BtLffVk4EtJ1q6q\nzw5aoU4AbmHVU+SaBVX12CSPoml9+kqSG4H1kzy4qm5oOd5c9yyamZozknyJZid7/6E9Plsk+aeV\nnXQ5+dFJ8gSaFqclg0eAnYEzkzyzqs5rM18P7Ay8A7ggySurylZjzSqLGvVSVV2dZH/g1CSbAs8H\nzq+q17QcrTeq6jvAW4G3JllI0w71zSTXVtWe7aabu6rqJOCkJA+guYfp1cBmST4MnFRVp7UacO77\nNc1iARq/twKLJrW5fi7J6cDbgD9pJVVPVNWtwKuTPJ5m1uZaYDkuJz8rmtXP+v3zKTffVC8lWdHT\nvjnwCeDLwPtWnHcTsNFK8oqq+ucpxgPsXVXe0zQiSe5XVXdOGnsQ8BzgoKpyr4gRSnJhVXlPTQuS\nXFlVU7aaJfluVT1y3Jn6JsmTgA8CpwLH0BQ1wMpXZtR986D5C2u/3b/Zdowp/fdX5rn5pjRC7x96\nfgmw2dCYm4CN3otoFgj4PdX8lMWCZrS+yaQb1Qd7o/zb4KHR2rztAD126yrO3T62FD2V5FM0y/U/\nr6oubTuP5h6LGvXSqlZ6SrLHOLNIY9bv/oT2ec9Ye7Zayf1Mwb2xxuGrVXXsVCeSbFZV/2/cgeaa\nvq9+ZlEj3dNngK3bDjHH7ZTklinGV/RWzx93oB7ZJMlK7x2rqqPHGaaH7Pluz+GrOLd0bCl6anJB\n46bLmm0WNdI9+ZPs0bu0qnZuO0RPzQMeiL/P27Klq5+1o6o+3naGvnPTZY2SRY10T/4kVXPZ9VX1\njrZD9Jirn7Ukyb+z8j/fq6oWjzNP37jp8oiV7WcWNeqlJJ9n6r/cAmw05jh9dELbAXrMGZp2/dQZ\ng9Z8YYqxrYFDaWYwNVr32HQ5iT9E1KyxqFFfHTXNc5odNyXZrqquGizj/DGa3uqrgYNdUnuknp7k\n/lV1BzQ7rAN/ClxTVZ9tN1ov/K7tAH1VVSeueJ5kG+CNNDMH7wE+2lauvnDTZY2aRY16aWX7oCTZ\nima3dZcVHq1XAccNni8CdgIeRrPj9AeBP2wnVi98ElgMXJXk4cC5wH8CByTZtare0Gq6ue9vh/bJ\nugcL+tFK8mjgTTR/1hwJvGzyvk0aHTddHp3YfmZRIyXZmGbjwUU0q6+c1G6iXrhzxUwBcADwiar6\nKc1P7963iq/TzP1BVV01eP4C4PiqemWStWju9bCoGa2jaFpfV7QBTm6/cY+sEUlyArCQ5tfg1cAy\nYH4zWXzXfk0ak6paCixNcjjND7qkGbGoUS8lWR94Bs00+CNoCpltqmrLVoP1x/Ikm9P0V+8HHDF0\nbt12IvXG8D+in0Tz02qq6ndJlk/9JZpFrwN+XFXXAyR5AXe3Xr69vVi9sCvN7//DgL8bjA0Xl9u0\nEarvqmp5klcDH2g7i7rNokZ9dSPNzupvBs6uqkryjJYz9clbafaFmAecXFWXAyTZB/hBm8F64JIk\nRwHXAQ+nWYmIJBu2mqo//hXYHyDJ3sC7gVcCjwOWAM9uL9rcVlUL2s6glXIBk1nQ9/azibYDSC15\nI7AO8GHgDUm2bTlPr1TVF4CHAo+uqpcMnVoKHNROqt54CXAzsAB4clX9ajD+GFwkYxzmDbU5HQQs\nqaoTq+otNEWmxijJtknelOSytrP0nKugacYsatRLVfWBqtqdZhOwAJ8DHpLkdUke0W66uS/JdsB/\nA19PcnySLQCq6vaquq3ddHNbVf26qt5TVa+qqouHxs+pqv9oM1tPzEuyoktiP5q9Olawe2IMkmye\n5NAk3wQup7nui1qONecluTXJLVM8bgUe0nY+dZ9/gKqXkhwKnA1cVFVHAEck2ZHmL7YvAs7cjNbH\ngE/Q7CJ9IPAh4JmtJuqJJGew6g0I9xtnnh46Hjgryc00G3F+HWCwEt0v2ww21yV5Cc2f8VsCnwFe\nDPzfqvr7VoP1RFWt33aGuaxZ/azfXXwWNeqrLYF/Ah6V5BLgHOAbwFFV9cZWk/XD+lV17OD5kUlc\nxnZ8DptibA/gtTT3mmmEquqIJF8FNgdOq6oVBeYEzb01Gp1jaJYwf95g5S3c/FGaOyxq1EtVdRjA\nYBnbhcCewIuAY5P8oqoe02a+Hlgnyc7cfXPousPH7tUxOlV1wYrng4UZ3gKsTbNfxxdbC9YjVXXe\nFGNXtpGlZx5Cs3z/0Uk2o5mtuX+7kSTNFosa9d26wHxgg8HjJ8ClrSbqhxuAo1dyXLhXx0gl+WOa\nYuY3wBFVdUbLkaSRq6qbaRaH+XCSLWk2Wr4xyRXASc7Sq+v6vvqZRY16KckSYHvgVuB8mvazo6vq\n560G64mq2rftDH2V5FvAJjT705w7GLtrh3tnyTRXJdljxSxZVV1Ls9rfUUkeSVPgSOowixr11dY0\nLTdX0ezXcS3wi1YT9UiSyYsCFM0ywxdV1a0tROqT24HbaPZDmbwnirNkmsv+Bdhl8mBVfRdwsQCp\n4yxq1EtV9ZQkoZmt2ZNmd+kdkvwMOLeq3tZqwLnvaVOMPQjYKcniqjp9ivOaBc6SSdIcVLafWdSo\ntwarDl2W5Bc0S6n+EjgA2A2wqBmhqnrhVONJHkpz8+7u403UH0kuplnO/BzgG1V1dbuJpLHZJsnJ\nKztZVQeOM4yk2WVRo15KcgjNDM1ewB00yzmfS7N/igsFtKSqrkniakSj9Rc0v/f/CHhbkgfQFDjn\nAOdU1flthpNG6Cbg/W2HkDQaFjXqqwU0O9q/uqqubzmLBgY37P627RxzWVVdBlwGLAFIsjHNTdKH\n0tw4Pa+9dNJI3VZVZ7UdQhqF2H5mUaN+qqrXtJ2hz5J8nnvuav8gmg0J/3L8ifojyTxgZ+6eqdyW\nZrGMjzBYDU2ao36e5MFVdQNAkr8CngVcA7y9qn7WajpJM2JRI6kNR006LuCnwFVV9bsW8vTJLcAV\nNLurv76qfthyHmlcNgR+B5Bkb+A9wCuBx9HMXE5eDVBSh1jUSBq7+9oCkuTcqnrCqPP0zIuBJwz+\n+8LBvjXn0qz6d12ryaTRmhiajTkIWFJVJwInJrmoxVzSrLD9TJLWXOu0HWCuqarjgeMBkqxHs9rf\nXsC7k6xVVQ9tM580QvdLcr+quhPYD3jp8LmWMkmaJf5PLGlNNvm+G82CwYpnu3P3fTW7Aj+mWQVQ\nmquOB85KcjPwa+DrAEkeTrOkv6QOs6iRpB5J8r/A1sBSmmWc3w+cV1W3tRpMGrGqOiLJV2kWJDlt\nsFcZwATNvTVSZ7n6mUWNpDVb2g4wB70AuHToH3RSb1TVeVOMXdlGFkmza6LtAJK0Cs9vO8BcU1WX\nANsn+XiSpUm+NXi+U9vZJEmaLosaSWOXZHGSw4eOr0tyS5Jbk7x8xfhgo0jNoiRPB04CzgJeRLMK\n2lk0K0A9vc1skqRpKpi4c818jIvtZ5La8DLgKUPHN1bVFknWAU4DPtxOrF54B/BHVXX10NjFSU4H\n/u/gIUlSpzhTI6kNE1X106HjEwCq6jfAuu1E6o37TypoABiM3X/saSRJmgXO1EhqwwbDB1X1LoAk\nE8BGrSTqjzuSbF1VPxoeTPJQYIyNApKk2TSxrN9r6zhTI6kNpyV55xTj76BpP9PovA34SpKDk+yY\nZIckL6S57m9tOZskSdPiTI2kNhwOfCTJ94CLB2OPpdk75cWtpeqBqvpckh8Cf0ezN0eAy4E/r6qL\nV/nFkiStoSxqJI1dVd0OLEqyDbD9YPjbVfX9FmP1xqB4+au2c0iSZoebb9p+JqkFSbZOsjXNPRwX\nDx53DI1rhJK8IMkFSW4fPJYmsciRJHWWMzWS2nAKUDStTysUsAmwKTCvjVB9MCheDgVeA1xI82uw\nC3BkEqrqE23mkyRpOixqJI1dVe04fJxkAfA6YH/gXS1E6pO/AZ4xaVnn05M8C/gUYFEjSR1k+5kk\ntSTJdkmOA74IXAA8pqo+1G6qOW/+KvapmT/2NJIkzQJnaiSNXZIdgDfRLBLwPmBxVfX8Z0xj8+tp\nnpMkaY1lUSOpDRcDP6a5t2Y3YLfk7ttrquqQlnL1waOTXDLFeIBtxh1GkjRzrn5mUSOpHYtpFgbQ\n+D267QCSJM02ixpJY1dVx7Wdoa+q6pr78rok51bVE0adR5Kk2WBRI2nsknyeVczUVNWBY4yjqa3T\ndgBJ0n1k+5lFjaRWHNV2AN0r2wMlSZ1hUSOpDWtV1ZenOpHkvcBZY84jSZI6zKJGUhuOSfLqqjpl\nxUCSCeBjwIPbi6UhufeXSJLWFLafSdL4PRn4UpK1q+qzSdYFTgBuAZ7WbjQNPL/tAJIk3VcTbQeQ\n1D+D3ev3B/4hycuArwBXVtXzquqOVsPNcUkWJzl86Pi6JLckuTXJy1eMV9Vl7SSUJGn1OVMjaeyS\n7DJ4+lrgE8CXgU+uGK+qC9vK1gMvA54ydHxjVW2RZB3gNODD7cSSJE2Xm29a1Ehqx/uHnl8CbDY0\nVsCTxp6oPyaq6qdDxycAVNVvBm2AkiR1jkWNpLGrqieu7FySPcaZpYc2GD6oqnfBXQs1bNRKIkmS\nZsiiRtKa5jPA1m2HmMNOS/LOqnrzpPF30LSfSZK6pmDizrZDtMuiRtKaxqWER+tw4CNJvgdcPBh7\nLLAUeHFrqSRJmgGLGklrGneyH6Gquh1YlGQbYPvB8Ler6vstxpIkaUYsaiSNXZLPM3XxEryvY6SS\nrGjtu5O7Z2ruGq+qH7WRS5I0fcHVzyxqJLXhqGme08ydQlNQDrf5FbAJsCkwr41QkiTNhEWNpLGr\nqrOmGk+yFfBcYMrzmrmq2nH4OMkC4HU0m6G+q4VIkiTNmEWNpFYl2Rh4DrAI2AI4qd1E/ZBkO+BN\nwO40ewQdUlV3tJtKkjQtbr5pUSNp/JKsDzwDeB7wCJpCZpuq2rLVYD2QZAeaYmZ74H3A4qrq+V+F\nkqSus6iR1IYbgW8CbwbOrqpK8oyWM/XFxcCPae6t2Q3YLbn79pqqOqSlXJKkHkryFOCDNPd0fqSq\n3jOd97GokdSGN9LcO/Nh4L+SfLrlPH2yGJfNlqQ5p4vtZ0nmAccAfwRcC3wryclV9e3VfS+LGklj\nV1UfAD4w2CtlEfA54CFJXgecVFVXthpwDquq49rOIEnSwG7A96rqBwBJPgU8HbCokbTmS3IocDZw\nUVUdARyRZEeaAueLwLZt5pvLVrFHEABVdeAY40iS+m0LmpboFa6lWcBmtVnUSGrDlsA/AY9Kcglw\nDvAN4KiqemOryeY+9wGSpDnmei449e1k47ZzrMQ6SZYOHS+pqiWD55ni9dNqkbaokTR2VXUYQJK1\ngIXAnsCLgGOT/KKqHtNmvjlurar68lQnkrwX9wiSpM6pqqe0nWGargW2GjreEvjJdN5oYlbiSNL0\nrAvMBzYYPH4CnN9qornvmCRPHR5IMpHkOOCx7USSJPXUt4Dtkjxs8IPO5wInT+eNnKmRNHZJltDs\nk3IrTRFzDnB0Vf281WD98GTgS0nWrqrPJlkXOAG4BXhau9EkSX1SVXcmeQVwKs2Szh+rqsun814W\nNZLasDWwNnAVcB3N9PMvWk3UE1V1dZL9gVOTbAo8Hzi/ql7TcjRJUg9V1f8A/zPT90mV2xVIGr80\nOz5uT3M/zZ7ADsDPgHOr6m1tZpvLkuwyeLo58Angy8D7VpyvqgvbyCVJ0kxY1EhqVZItgb1oCpsD\ngI2qasN2U81dSc5YxemqqieNLYwkSbPEokbS2CU5hKaI2Qu4g2Y553MH/720qpa3GK+3kuxRVee1\nnUOSpNVlUSNp7JIczWBvmqq6vu08aiT5UVVt3XYOSZJWl0WNJAmAJD+uqq3u/ZWSJK1Z3KdGkrSC\nP+WSJHWSSzpLUo8k+TxTFy8BNhpzHEmSZoXtZ5LUI0n2WdX5qjprXFkkSZotFjWSJJJsBTy3qo5s\nO4skSavLe2okqaeSbJzk5Um+BpwJbNZyJEmSpsV7aiSpR5KsDzwDeB7wCOAkYJuq2rLVYJIkzYDt\nZ5LUI0l+DXwTeDNwdlVVkh9U1TYtR5MkadpsP5OkfnkjsA7wYeANSbZtOY8kSTPmTI0k9VCSbYBF\nwHOB7YC3ASdV1ZWtBpMkaRosaiSpR5IcCpwNXFRVdw7GdqQpcA6qKmduJEmdY1EjST2S5ChgT+BR\nwCXAOcA3gHOr6mdtZpMkabosaiSph5KsBSykKXCeMHj8oqoe02owSZKmwSWdJamf1gXmAxsMHj8B\nLm01kSRJ0+RMjST1SJIlwPbArcD5wHnAeVX181aDSZI0Ay7pLEn9sjWwNnADcB1wLfCLVhNJkjRD\nztRIUs8kCc1szZ6Dxw7Az2gWC3hbm9kkSZoOixpJ6qkkWwJ70RQ2BwAbVdWG7aaSJGn1WdRIUo8k\nOYSmiNkLuIPBcs6D/15aVctbjCdJ0rS4+pkk9csC4L+BV1fV9S1nkSRpVjhTI0mSJKnTXP1MkiRJ\nUqdZ1EiSJEnqNIsaSdJqSbIsyUVJLktyQpL1ZvBe+yb5wuD5gUlev4rXbpjkb6bxGW9Pcth9HZ/0\nmuOSPHs1PmtBkstWN6MkaWYsaiRJq+vXVfW4qtoB+B3wsuGTaaz23y9VdXJVvWcVL9kQWO2iRpI0\n91nUSJJm4uvAwwczFFck+RfgQmCrJE9Ocm6SCwczOg8ESPKUJN9JcjbwzBVvlOTgJP88eL5ZkpOS\nXDx47Am8B9h2MEt05OB1hyf5VpJLkvz90Hu9Kcl3k3wFeOS9fRNJXjJ4n4uTnDhp9mn/JF9PcmWS\nAwavn5fkyKHP/uuZXkhJ0vRZ1EiSpiXJ/YA/AS4dDD0S+ERV7QzcDrwZ2L+qdgGWAq9Jsg5wLPA0\n4A+BB6/k7f8JOKuqHgvsAlwOvB74/mCW6PAkTwa2A3YDHgc8PsneSR4PPBfYmaZo2vU+fDufrapd\nB593BbB46NwCYB/gqcC/Dr6HxcAvq2rXwfu/JMnD7sPnSJJGwH1qJEmra90kFw2efx34KPAQ4Jqq\nOm8wvgfwGOAbSQDWotnk81HAD6vqKoAknwReOsVnPAn4K4CqWgb8MskfTHrNkweP/x0cP5CmyFkf\nOKmqfjX4jJPvw/e0Q5J30rS4PRA4dejcZwabkl6V5AeD7+HJwE5D99tsMPjsK+/DZ0mSZplFjSRp\ndf26qh43PDAoXG4fHgK+XFWLJr3uccBsbZAW4N1V9W+TPuPQaXzGccCfVdXFSQ4G9h06N/m9avDZ\nr6yq4eKHJAtW83MlSbPA9jNJ0iicB+yV5OEASdZL8gjgO8DDkmw7eN2ilXz9V4GXD752XpL5wK00\nszArnAq8aOhenS2SbAp8DXhGknWTrE/T6nZv1geuT3J/4C8mnXtOkolB5m2A7w4+++WD15PkEUke\ncB8+R5I0As7USJJmXVXdNJjxOD7J2oPhN1fVlUleCpyS5GbgbGCHKd7iVcCSJIuBZcDLq+rcJN8Y\nLJn8xcF9NY8Gzh3MFN0G/GVVXZjk08BFwDU0LXL35i3A+YPXX8rvF0/fBc4CNgNeVlW/SfIRmntt\nLkzz4TcBf3bfro4kabalara6ACRJkiRp/Gw/kyRJktRpFjWSJEmSOs2iRpIkSVKnWdRIkiRJ6jSL\nGkmSJEmdZlEjSZIkqdMsaiRJkiR1mkWNJEmSpE77/9jBq1tvhV1QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26c7a527710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "predictions = y_pre\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = metrics.confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\n",
    "print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion matrix (normalised to % of total test data):\")\n",
    "print(normalised_confusion_matrix)\n",
    "print(\"Note: training and testing data is not equally distributed amongst classes, \")\n",
    "print(\"so it is normal that more than a 6th of the data is correctly classifier in the last category.\")\n",
    "%matplotlib inline\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.rainbow\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
